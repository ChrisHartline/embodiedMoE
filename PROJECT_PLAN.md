# Clara v2 - Embodied AI Project Plan

## Overview

Clara is an embodied AI system with:
- **Dual-brain architecture**: Knowledge (Phi-3) + Personality (Mistral+LoRA)
- **Hyperdimensional Computing (HDC) memory**: 64k-dim bipolar vectors
- **Intelligent routing**: Nemotron-Orchestrator-8B
- **Multi-tier memory stack**: Session cache â†’ HDC â†’ Graph/Vector DB

---

## Memory Stack Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLARA MEMORY STACK                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  L1: SESSION CACHE (FalkorDB/Redis)     âš¡ <1ms                 â”‚
â”‚  â”œâ”€ Current conversation turns                                  â”‚
â”‚  â”œâ”€ Active entities & slots                                     â”‚
â”‚  â”œâ”€ Hot routing decisions                                       â”‚
â”‚  â””â”€ Working memory window                                       â”‚
â”‚                                                                 â”‚
â”‚  L2: HDC MEMORY (64k bipolar)           ğŸ§  ~5ms                 â”‚
â”‚  â”œâ”€ Semantic similarity search                                  â”‚
â”‚  â”œâ”€ Associative binding (âŠ—)  â†â”€â”€ QC bridge                     â”‚
â”‚  â”œâ”€ Episode bundles                                             â”‚
â”‚  â””â”€ Domain-specific bundles                                     â”‚
â”‚                                                                 â”‚
â”‚  L3: GRAPH + VECTOR (FalkorDB)          ğŸ“š ~20ms                â”‚
â”‚  â”œâ”€ Entity relationships (graph)                                â”‚
â”‚  â”œâ”€ Long-term episodic memory (vectors)                         â”‚
â”‚  â”œâ”€ Concept ontology                                            â”‚
â”‚  â””â”€ Cross-session knowledge                                     â”‚
â”‚                                                                 â”‚
â”‚  L4: STRUCTURED STORE (SQLite)          ğŸ—„ï¸ ~50ms               â”‚
â”‚  â”œâ”€ User preferences                                            â”‚
â”‚  â”œâ”€ Configuration                                               â”‚
â”‚  â””â”€ Audit logs                                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L1: Session â”‚ â† Check active entities, recent turns
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L2: HDC 64k â”‚ â† Semantic recall via binding/similarity
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ (cache miss or need relationships?)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L3: FalkorDBâ”‚ â† Graph traversal + vector search
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
  Router â†’ Brain â†’ Response
    â”‚
    â–¼
  Store: L1 (sync), L2 (sync), L3 (async)
```

---

## HDC â†’ Quantum Computing Bridge

### Why HDC as a QC Bridge?

Hyperdimensional Computing provides a **classical approximation** of quantum-like operations that can later migrate to actual quantum hardware. The mathematical structures align:

### Operation Mapping

| HDC Operation | Mathematical Form | Quantum Analog | Description |
|---------------|-------------------|----------------|-------------|
| **Bind (âŠ—)** | `A âŠ— B = A * B` (element-wise) | Entanglement / Tensor product | Creates composite representation dissimilar to both inputs |
| **Bundle (+)** | `sign(Î£ Aáµ¢)` | Superposition | Multiple states coexist in single vector |
| **Permute (Ï)** | `roll(A, n)` | Phase rotation | Encodes position/sequence information |
| **Similarity** | `(A Â· B) / D` | Measurement | "Collapses" to nearest stored pattern |
| **Unbind** | `A âŠ— B âŠ— B = A` | Disentanglement | Recovers original from bound pair |

### Key Parallels

#### 1. Superposition via Bundling
```
Classical:  |stateâŸ© = Î±|0âŸ© + Î²|1âŸ©

HDC:        bundle = sign(wâ‚Â·hvâ‚ + wâ‚‚Â·hvâ‚‚ + ... + wâ‚™Â·hvâ‚™)
            - Each hváµ¢ is quasi-orthogonal (random in high-D)
            - Weighted sum preserves similarity to all components
            - "Measurement" = find most similar stored pattern
```

#### 2. Entanglement via Binding
```
Quantum:    |ÏˆâŸ© = |AâŸ© âŠ— |BâŸ©  (tensor product, entangled state)

HDC:        bound = bind(A, B) = A * B  (element-wise)
            - Result is dissimilar to both A and B
            - But: unbind(bound, B) â‰ˆ A (recovers original)
            - Creates "associated" representation
```

#### 3. Interference via Similarity
```
Quantum:    Probability amplitudes interfere constructively/destructively

HDC:        Similar patterns reinforce in bundles
            Dissimilar patterns cancel out (noise)
            High-D ensures random vectors are ~orthogonal
```

### The 64k Dimension Choice

```
D = 64,000 dimensions provides:

1. Orthogonality:  E[sim(randomâ‚, randomâ‚‚)] â‰ˆ 0
                   Var[sim] â‰ˆ 1/D = 1/64000 â‰ˆ 0.0000156

2. Noise resistance: Error tolerance scales with âˆšD
                     âˆš64000 â‰ˆ 253 (vs âˆš10000 â‰ˆ 100)

3. Bundle capacity:  ~âˆšD items before interference
                     ~253 items per bundle

4. Quantum-ready:    Maps to 64k qubit register (future)
```

### Quantum-Inspired Algorithms (Future)

#### Grover-like Search
```python
# Classical HDC approximation of Grover's algorithm
def quantum_inspired_search(query_hv, memory_bundle, iterations=3):
    """
    Amplitude amplification via iterative refinement
    """
    current = query_hv.copy()

    for _ in range(iterations):
        # "Oracle" - identify matching components
        similarities = [hdc.similarity(current, mem) for mem in memories]

        # "Diffusion" - amplify high-similarity, suppress low
        weights = softmax(similarities * temperature)
        current = hdc.bundle(memories, weights)

    return current  # Amplified toward best match
```

#### Quantum Annealing for Optimization
```python
# Energy-based memory consolidation (sleep/dreaming)
def consolidate_memories(memories, temperature_schedule):
    """
    Simulated annealing over HDC space
    Similar memories cluster, redundant ones merge
    """
    for T in temperature_schedule:  # Cool down
        for i, mem in enumerate(memories):
            # Find neighbors
            neighbors = find_similar(mem, threshold=T)

            # Probabilistic merge (Boltzmann)
            if random() < exp(-energy_diff / T):
                memories[i] = bundle(neighbors)

    return deduplicate(memories)
```

### Migration Path to Quantum Hardware

```
Phase 1 (Current):  Classical HDC on CPU/GPU
                    64k float32/int8 vectors

Phase 2 (Near):     Quantum-inspired on classical
                    Tensor network approximations
                    GPU-accelerated similarity search

Phase 3 (Future):   Hybrid classical-quantum
                    Quantum similarity search (Grover)
                    Quantum bundling (superposition)
                    Classical binding (still efficient)

Phase 4 (Far):      Full quantum HDC
                    64k qubit register
                    Native superposition/entanglement
                    Exponential speedup for search
```

---

## Component Status

### Implemented âœ…

| Component | File | Status |
|-----------|------|--------|
| HDC Memory 64k | `hdc_memory_64k.py` | âœ… Complete |
| Nemotron Router | `nemotron_router.py` | âœ… Complete |
| Clara v2 Integration | `clara_v2.py` | âœ… Complete |
| Embedding Router | `nemotron_router.py` | âœ… Complete |
| Hybrid Router | `nemotron_router.py` | âœ… Complete |

### In Progress ğŸ”„

| Component | File | Status |
|-----------|------|--------|
| Session Memory (FalkorDB) | `session_memory.py` | ğŸ”„ Next |
| Graph Memory Layer | TBD | ğŸ”„ Planned |

### Planned ğŸ“‹

| Component | Description | Priority |
|-----------|-------------|----------|
| Voice LoRA Adapter | Fine-tune personality from chat history | High |
| Tool Execution | Actual tool calling via Orchestrator | Medium |
| Sleep/Consolidation | Memory consolidation during idle | Medium |
| Quantum-Inspired Search | Grover-like amplitude amplification | Low |

---

## FalkorDB Integration

### Why FalkorDB?

FalkorDB provides:
1. **Redis-compatible** - Fast key-value for session cache
2. **Graph database** - Cypher queries for relationships
3. **Vector search** - Similarity search for embeddings
4. **Single container** - Simplifies deployment

### Graph Schema (Clara)

```cypher
// Nodes
(:User {id, name, preferences})
(:Entity {name, type, first_seen, last_seen})
(:Memory {id, text, timestamp, domain, importance})
(:Concept {name, domain})
(:Session {id, started, ended})

// Relationships
(:User)-[:HAS_SESSION]->(:Session)
(:Session)-[:CONTAINS]->(:Memory)
(:Memory)-[:MENTIONS]->(:Entity)
(:Memory)-[:RELATES_TO]->(:Concept)
(:Entity)-[:CONNECTED_TO]->(:Entity)
(:Concept)-[:SUBCONCEPT_OF]->(:Concept)
```

### Usage Pattern

```python
# Session cache (Redis protocol)
await falkor.set(f"session:{sid}:turns", json.dumps(turns))
await falkor.get(f"session:{sid}:entities")

# Graph queries (Cypher)
result = await falkor.graph.query("""
    MATCH (m:Memory)-[:MENTIONS]->(e:Entity {name: $entity})
    WHERE m.timestamp > $since
    RETURN m.text, m.domain
    ORDER BY m.timestamp DESC
    LIMIT 5
""", {"entity": "coffee", "since": yesterday})

# Vector search
similar = await falkor.graph.query("""
    CALL db.idx.vector.queryNodes('Memory', 'embedding', 5, $query_vec)
    YIELD node, score
    RETURN node.text, score
""", {"query_vec": query_embedding})
```

---

## Voice Adapter (Planned)

### Data Requirements
- ~100k tokens of conversational data
- Consistent persona/style throughout
- Mix of topics and emotional tones

### Training Config
```python
lora_config = {
    "r": 16,
    "lora_alpha": 32,
    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
    "lora_dropout": 0.05,
}

training = {
    "base_model": "mistralai/Mistral-Nemo-Base-2407",  # or similar
    "epochs": 3-5,
    "lr": 1e-4,
    "batch_size": 4,
}
```

---

## References

- [HDC Tutorial](https://www.hd-computing.com/)
- [Nemotron-Orchestrator-8B](https://huggingface.co/nvidia/Nemotron-Orchestrator-8B)
- [FalkorDB Docs](https://docs.falkordb.com/)
- [ToolOrchestra Paper](https://arxiv.org/abs/2511.21689)
