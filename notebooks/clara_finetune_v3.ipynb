{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLARA FINE-TUNING - Fixed for Mistral & Phi-3 (v3)\n",
        "\n",
        "**Fixes:**\n",
        "- Separate training cells for Mistral and Phi-3\n",
        "- Phi-3 gradient checkpointing compatibility fix\n",
        "- Conservative hyperparameters\n",
        "\n",
        "**Strategy:**\n",
        "- **Personality** (warmth, playful, formal, encouragement) â†’ **Mistral 7B**\n",
        "- **Knowledge** (medical, coding, teaching, quantum) â†’ **Phi-3-mini**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.40.0 datasets accelerate wandb bitsandbytes\n",
        "!pip install -q peft trl sentencepiece\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "print(f\"âœ“ Setup complete!\")\n",
        "print(f\"Transformers: {transformers.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Configuration - CHANGE DIMENSION HERE!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# WHICH DIMENSION ARE YOU TRAINING?\n",
        "# ============================================================\n",
        "\n",
        "DIMENSION = \"medical\"  # <-- CHANGE THIS EACH RUN\n",
        "\n",
        "# ============================================================\n",
        "# AUTO-CONFIGURATION\n",
        "# ============================================================\n",
        "PERSONALITY_DIMS = [\"warmth\", \"playful\", \"formal\", \"encouragement\"]\n",
        "KNOWLEDGE_DIMS = [\"medical\", \"coding\", \"teaching\", \"quantum\"]\n",
        "\n",
        "if DIMENSION in PERSONALITY_DIMS:\n",
        "    BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    DATA_PATH = f\"/content/drive/MyDrive/Lily/training_data/{DIMENSION}_training.json\"\n",
        "    model_prefix = \"mistral\"\n",
        "    TRAINING_TYPE = \"Personality\"\n",
        "    USE_PHI3 = False\n",
        "elif DIMENSION in KNOWLEDGE_DIMS:\n",
        "    BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "    DATA_PATH = f\"/content/drive/MyDrive/Lily/training_data/{DIMENSION}_knowledge.json\"\n",
        "    model_prefix = \"phi3\"\n",
        "    TRAINING_TYPE = \"Knowledge\"\n",
        "    USE_PHI3 = True\n",
        "else:\n",
        "    raise ValueError(f\"Unknown dimension: {DIMENSION}\")\n",
        "\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/Lily/models/{model_prefix}_{DIMENSION}\"\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING CONFIG - CONSERVATIVE\n",
        "# ============================================================\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUM = 4\n",
        "LEARNING_RATE = 5e-5\n",
        "MAX_LENGTH = 512\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"ðŸŽ¯ TRAINING: {DIMENSION.upper()}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Type:   {TRAINING_TYPE}\")\n",
        "print(f\"  Model:  {BASE_MODEL}\")\n",
        "print(f\"  Phi-3:  {USE_PHI3}\")\n",
        "print(f\"  LR:     {LEARNING_RATE}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if USE_PHI3:\n",
        "    print(\"\\nâš ï¸  PHI-3 DETECTED: Run Cell 9-Phi (skip Cell 9-Mistral)\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  MISTRAL DETECTED: Run Cell 9-Mistral (skip Cell 9-Phi)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nâœ“ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"No GPU!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from datasets import Dataset\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    print(f\"âŒ File not found: {DATA_PATH}\")\n",
        "    data_dir = \"/content/drive/MyDrive/Lily/training_data\"\n",
        "    if os.path.exists(data_dir):\n",
        "        print(\"\\nAvailable:\")\n",
        "        for f in sorted(os.listdir(data_dir)):\n",
        "            print(f\"  - {f}\")\n",
        "    raise FileNotFoundError(DATA_PATH)\n",
        "\n",
        "with open(DATA_PATH) as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"âœ“ Loaded {len(raw_data)} examples\")\n",
        "\n",
        "def format_data(examples, dimension, training_type):\n",
        "    formatted = []\n",
        "    for ex in examples:\n",
        "        if training_type == \"Personality\":\n",
        "            if 'neutral' in ex and 'high' in ex:\n",
        "                formatted.append({\n",
        "                    \"instruction\": f\"Rewrite this with high {dimension}: {ex['neutral']}\",\n",
        "                    \"response\": ex['high']\n",
        "                })\n",
        "            if 'neutral' in ex and 'low' in ex:\n",
        "                formatted.append({\n",
        "                    \"instruction\": f\"Rewrite this with low {dimension}: {ex['neutral']}\",\n",
        "                    \"response\": ex['low']\n",
        "                })\n",
        "        else:\n",
        "            if 'question' in ex and 'answer' in ex:\n",
        "                formatted.append({\n",
        "                    \"instruction\": ex['question'],\n",
        "                    \"response\": ex['answer']\n",
        "                })\n",
        "    return formatted\n",
        "\n",
        "formatted = format_data(raw_data, DIMENSION, TRAINING_TYPE)\n",
        "print(f\"âœ“ Formatted: {len(formatted)} examples\")\n",
        "\n",
        "dataset = Dataset.from_list(formatted)\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "print(f\"  Train: {len(dataset['train'])}, Val: {len(dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(f\"Loading {BASE_MODEL}...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\",\n",
        "    use_cache=False  # Important for training\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"âœ“ Loaded: {model.num_parameters():,} params\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Different target modules for different models\n",
        "if USE_PHI3:\n",
        "    target_modules = [\"qkv_proj\", \"o_proj\"]  # Phi-3 uses combined qkv\n",
        "else:\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"âœ“ LoRA configured\")\n",
        "print(f\"  Target modules: {target_modules}\")\n",
        "print(f\"  Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt(instruction, response=\"\"):\n",
        "    if response:\n",
        "        return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
        "    return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    prompts = [create_prompt(i, r) for i, r in zip(examples['instruction'], examples['response'])]\n",
        "    tokenized = tokenizer(prompts, truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing...\")\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=dataset['train'].column_names)\n",
        "print(\"âœ“ Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Phi-3 needs gradient checkpointing disabled due to cache issues\n",
        "use_gradient_checkpointing = not USE_PHI3\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # Training\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUM,\n",
        "    \n",
        "    # Optimizer\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    \n",
        "    # Precision\n",
        "    bf16=True,\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,\n",
        "    \n",
        "    # Saving\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    \n",
        "    # Eval\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    \n",
        "    # W&B\n",
        "    report_to=\"wandb\",\n",
        "    run_name=f\"clara-{model_prefix}-{DIMENSION}-v3\",\n",
        "    \n",
        "    # Stability - KEY DIFFERENCE FOR PHI-3\n",
        "    gradient_checkpointing=use_gradient_checkpointing,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "\n",
        "print(\"âœ“ Training config\")\n",
        "print(f\"  Gradient checkpointing: {use_gradient_checkpointing}\")\n",
        "print(f\"  LR: {LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9-Mistral: Train Mistral (Personality)\n",
        "\n",
        "**Run this cell for:** warmth, playful, formal, encouragement\n",
        "\n",
        "**Skip this cell for:** medical, coding, teaching, quantum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MISTRAL TRAINING (with gradient checkpointing)\n",
        "# ============================================================\n",
        "\n",
        "if USE_PHI3:\n",
        "    print(\"âš ï¸  SKIP THIS CELL - You're training Phi-3!\")\n",
        "    print(\"   Run Cell 9-Phi instead.\")\n",
        "else:\n",
        "    from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized['train'],\n",
        "        eval_dataset=tokenized['test'],\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸš€ TRAINING MISTRAL: {DIMENSION.upper()}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  Watch: https://wandb.ai/chris_hartline/clara-deng-research\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    trainer.train()\n",
        "    \n",
        "    print(\"\\nâœ“ Mistral training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9-Phi: Train Phi-3 (Knowledge)\n",
        "\n",
        "**Run this cell for:** medical, coding, teaching, quantum\n",
        "\n",
        "**Skip this cell for:** warmth, playful, formal, encouragement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PHI-3 TRAINING (without gradient checkpointing)\n",
        "# ============================================================\n",
        "\n",
        "if not USE_PHI3:\n",
        "    print(\"âš ï¸  SKIP THIS CELL - You're training Mistral!\")\n",
        "    print(\"   Run Cell 9-Mistral instead.\")\n",
        "else:\n",
        "    from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "    \n",
        "    # Ensure cache is disabled for training\n",
        "    model.config.use_cache = False\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized['train'],\n",
        "        eval_dataset=tokenized['test'],\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸš€ TRAINING PHI-3: {DIMENSION.upper()}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  Note: Gradient checkpointing DISABLED for Phi-3 compatibility\")\n",
        "    print(f\"  Watch: https://wandb.ai/chris_hartline/clara-deng-research\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    trainer.train()\n",
        "    \n",
        "    print(\"\\nâœ“ Phi-3 training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Saving to: {OUTPUT_DIR}\")\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Verify save\n",
        "required_files = ['adapter_config.json', 'adapter_model.safetensors']\n",
        "saved_files = os.listdir(OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\nSaved files:\")\n",
        "for f in saved_files:\n",
        "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f)) / 1e6\n",
        "    print(f\"  - {f} ({size:.1f} MB)\")\n",
        "\n",
        "# Check required files exist\n",
        "if 'adapter_config.json' in saved_files:\n",
        "    print(\"\\nâœ… Model saved correctly!\")\n",
        "else:\n",
        "    print(\"\\nâŒ WARNING: adapter_config.json missing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING TRAINED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Enable cache for inference\n",
        "model.eval()\n",
        "model.config.use_cache = True\n",
        "\n",
        "# Disable gradient checkpointing for inference\n",
        "if hasattr(model, 'gradient_checkpointing_disable'):\n",
        "    model.gradient_checkpointing_disable()\n",
        "if hasattr(model.base_model, 'gradient_checkpointing_disable'):\n",
        "    model.base_model.gradient_checkpointing_disable()\n",
        "\n",
        "def generate_response(prompt, max_new_tokens=100):\n",
        "    full_prompt = create_prompt(prompt)\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.split(\"### Response:\")[-1].strip()\n",
        "    return response\n",
        "\n",
        "# Test prompts\n",
        "if TRAINING_TYPE == \"Personality\":\n",
        "    test_prompts = [\n",
        "        f\"Rewrite this with high {DIMENSION}: I can help you with that.\",\n",
        "        f\"Rewrite this with high {DIMENSION}: That's correct.\",\n",
        "    ]\n",
        "else:\n",
        "    test_prompts = [\n",
        "        f\"Explain a basic concept in {DIMENSION}.\",\n",
        "        f\"What should a beginner know about {DIMENSION}?\",\n",
        "    ]\n",
        "\n",
        "print(\"\\nResults:\\n\")\n",
        "all_ok = True\n",
        "for prompt in test_prompts:\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Response: {response}\\n\")\n",
        "    \n",
        "    if \"Question:\" in response or \"}}\" in response or \"QR\" in response:\n",
        "        print(\"âš ï¸ WARNING: Garbled output!\")\n",
        "        all_ok = False\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\nâœ… Model looks good!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Model may need retraining\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12: Progress & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()\n",
        "\n",
        "import os\n",
        "models_dir = \"/content/drive/MyDrive/Lily/models\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING PROGRESS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_dims = PERSONALITY_DIMS + KNOWLEDGE_DIMS\n",
        "completed = 0\n",
        "\n",
        "for dim in all_dims:\n",
        "    prefix = \"mistral\" if dim in PERSONALITY_DIMS else \"phi3\"\n",
        "    path = f\"{models_dir}/{prefix}_{dim}\"\n",
        "    \n",
        "    # Check if properly saved\n",
        "    if os.path.exists(path) and os.path.exists(f\"{path}/adapter_config.json\"):\n",
        "        icon = \"âœ…\"\n",
        "        completed += 1\n",
        "    elif os.path.exists(path):\n",
        "        icon = \"âš ï¸\"  # Exists but may be incomplete\n",
        "    else:\n",
        "        icon = \"â¬œ\"\n",
        "    \n",
        "    model_type = \"Mistral\" if dim in PERSONALITY_DIMS else \"Phi-3\"\n",
        "    print(f\"  {icon} {dim} ({model_type})\")\n",
        "\n",
        "print(f\"\\nCompleted: {completed}/8\")\n",
        "\n",
        "if completed < 8:\n",
        "    remaining = []\n",
        "    for d in all_dims:\n",
        "        prefix = \"mistral\" if d in PERSONALITY_DIMS else \"phi3\"\n",
        "        path = f\"{models_dir}/{prefix}_{d}\"\n",
        "        if not os.path.exists(path) or not os.path.exists(f\"{path}/adapter_config.json\"):\n",
        "            remaining.append(d)\n",
        "    \n",
        "    if remaining:\n",
        "        next_dim = remaining[0]\n",
        "        next_type = \"Mistral\" if next_dim in PERSONALITY_DIMS else \"Phi-3\"\n",
        "        print(f\"\\nðŸ‘‰ Next: DIMENSION = '{next_dim}' ({next_type})\")\n",
        "        print(f\"   Change Cell 2 and run all cells again.\")\n",
        "else:\n",
        "    print(\"\\nðŸŽ‰ ALL TRAINING COMPLETE!\")\n",
        "    print(\"\\nNext: Run mergekit to combine models into Clara!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
