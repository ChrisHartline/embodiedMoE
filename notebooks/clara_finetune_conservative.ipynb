{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLARA FINE-TUNING - Conservative Settings (v2)\n",
        "\n",
        "**Fixed issues from v1:**\n",
        "- Lower learning rate (5e-5 instead of 2e-4)\n",
        "- Fewer epochs (1 instead of 3)\n",
        "- Higher max_grad_norm (1.0 instead of 0.3)\n",
        "- Proper inference mode for testing\n",
        "\n",
        "**Strategy:**\n",
        "- **Personality** â†’ **Mistral 7B**\n",
        "- **Knowledge** â†’ **Phi-3-mini**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate wandb bitsandbytes\n",
        "!pip install -q peft trl sentencepiece\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "print(f\"âœ“ Setup complete!\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Configuration - CHANGE DIMENSION HERE!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# WHICH DIMENSION ARE YOU TRAINING?\n",
        "# ============================================================\n",
        "\n",
        "DIMENSION = \"warmth\"  # <-- CHANGE THIS EACH RUN\n",
        "\n",
        "# ============================================================\n",
        "# AUTO-CONFIGURATION (don't edit below)\n",
        "# ============================================================\n",
        "PERSONALITY_DIMS = [\"warmth\", \"playful\", \"formal\", \"encouragement\"]\n",
        "KNOWLEDGE_DIMS = [\"medical\", \"coding\", \"teaching\", \"quantum\"]\n",
        "\n",
        "if DIMENSION in PERSONALITY_DIMS:\n",
        "    BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    DATA_PATH = f\"/content/drive/MyDrive/Lily/training_data/{DIMENSION}_training.json\"\n",
        "    model_prefix = \"mistral\"\n",
        "    TRAINING_TYPE = \"Personality\"\n",
        "elif DIMENSION in KNOWLEDGE_DIMS:\n",
        "    BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "    DATA_PATH = f\"/content/drive/MyDrive/Lily/training_data/{DIMENSION}_knowledge.json\"\n",
        "    model_prefix = \"phi3\"\n",
        "    TRAINING_TYPE = \"Knowledge\"\n",
        "else:\n",
        "    raise ValueError(f\"Unknown dimension: {DIMENSION}\")\n",
        "\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/Lily/models/{model_prefix}_{DIMENSION}\"\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING CONFIG - CONSERVATIVE SETTINGS\n",
        "# ============================================================\n",
        "EPOCHS = 1              # Reduced from 3\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUM = 4\n",
        "LEARNING_RATE = 5e-5    # Reduced from 2e-4 (4x lower)\n",
        "MAX_LENGTH = 512\n",
        "MAX_GRAD_NORM = 1.0     # Increased from 0.3\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"ðŸŽ¯ TRAINING: {DIMENSION.upper()} (Conservative Settings)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Type:          {TRAINING_TYPE}\")\n",
        "print(f\"  Model:         {BASE_MODEL}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE} (conservative)\")\n",
        "print(f\"  Epochs:        {EPOCHS}\")\n",
        "print(f\"  Output:        {OUTPUT_DIR}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nâœ“ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"No GPU! Go to Runtime â†’ Change runtime type â†’ A100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from datasets import Dataset\n",
        "\n",
        "# Check file exists\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    print(f\"âŒ File not found: {DATA_PATH}\")\n",
        "    print(\"\\nAvailable files:\")\n",
        "    data_dir = \"/content/drive/MyDrive/Lily/training_data\"\n",
        "    if os.path.exists(data_dir):\n",
        "        for f in sorted(os.listdir(data_dir)):\n",
        "            print(f\"  - {f}\")\n",
        "    raise FileNotFoundError(DATA_PATH)\n",
        "\n",
        "# Load data\n",
        "with open(DATA_PATH) as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"âœ“ Loaded {len(raw_data)} examples\")\n",
        "\n",
        "# Format for training\n",
        "def format_data(examples, dimension, training_type):\n",
        "    formatted = []\n",
        "    for ex in examples:\n",
        "        if training_type == \"Personality\":\n",
        "            if 'neutral' in ex and 'high' in ex:\n",
        "                formatted.append({\n",
        "                    \"instruction\": f\"Rewrite this with high {dimension}: {ex['neutral']}\",\n",
        "                    \"response\": ex['high']\n",
        "                })\n",
        "            if 'neutral' in ex and 'low' in ex:\n",
        "                formatted.append({\n",
        "                    \"instruction\": f\"Rewrite this with low {dimension}: {ex['neutral']}\",\n",
        "                    \"response\": ex['low']\n",
        "                })\n",
        "        else:\n",
        "            if 'question' in ex and 'answer' in ex:\n",
        "                formatted.append({\n",
        "                    \"instruction\": ex['question'],\n",
        "                    \"response\": ex['answer']\n",
        "                })\n",
        "    return formatted\n",
        "\n",
        "formatted = format_data(raw_data, DIMENSION, TRAINING_TYPE)\n",
        "print(f\"âœ“ Formatted: {len(formatted)} training examples\")\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_list(formatted)\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "print(f\"  Train: {len(dataset['train'])}, Val: {len(dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(f\"Loading {BASE_MODEL}...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"âœ“ Model loaded: {model.num_parameters():,} parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.1,  # Increased from 0.05 for regularization\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"âœ“ LoRA: {trainable:,} trainable ({100*trainable/total:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt(instruction, response=\"\"):\n",
        "    if response:\n",
        "        return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
        "    return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    prompts = [create_prompt(i, r) for i, r in zip(examples['instruction'], examples['response'])]\n",
        "    tokenized = tokenizer(prompts, truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing...\")\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=dataset['train'].column_names)\n",
        "print(\"âœ“ Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Training Config (CONSERVATIVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # Training - CONSERVATIVE\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUM,\n",
        "    \n",
        "    # Optimizer - LOWER LR\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    \n",
        "    # Precision\n",
        "    bf16=True,\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,\n",
        "    \n",
        "    # Saving\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    \n",
        "    # Eval\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    \n",
        "    # W&B\n",
        "    report_to=\"wandb\",\n",
        "    run_name=f\"clara-{model_prefix}-{DIMENSION}-v2\",\n",
        "    \n",
        "    # Stability\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "\n",
        "print(\"âœ“ Training config (CONSERVATIVE)\")\n",
        "print(f\"  LR: {LEARNING_RATE} (4x lower than default)\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Max grad norm: {MAX_GRAD_NORM}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized['train'],\n",
        "    eval_dataset=tokenized['test'],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"ðŸš€ TRAINING: {DIMENSION.upper()} (Conservative)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Watch: https://wandb.ai/chris_hartline/clara-deng-research\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nâœ“ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Saving to: {OUTPUT_DIR}\")\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Size\n",
        "total_size = sum(\n",
        "    os.path.getsize(os.path.join(OUTPUT_DIR, f)) \n",
        "    for f in os.listdir(OUTPUT_DIR) \n",
        "    if os.path.isfile(os.path.join(OUTPUT_DIR, f))\n",
        ")\n",
        "print(f\"âœ“ Saved ({total_size/1e6:.1f} MB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Test Model (Fixed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING TRAINED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# IMPORTANT: Disable gradient checkpointing for inference\n",
        "model.eval()\n",
        "model.config.use_cache = True\n",
        "if hasattr(model, 'gradient_checkpointing_disable'):\n",
        "    model.gradient_checkpointing_disable()\n",
        "\n",
        "def generate_response(prompt, max_new_tokens=100):\n",
        "    full_prompt = create_prompt(prompt)\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1  # Helps prevent repetition\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.split(\"### Response:\")[-1].strip()\n",
        "    return response\n",
        "\n",
        "# Test prompts\n",
        "if TRAINING_TYPE == \"Personality\":\n",
        "    test_prompts = [\n",
        "        f\"Rewrite this with high {DIMENSION}: I can help you with that.\",\n",
        "        f\"Rewrite this with high {DIMENSION}: That's correct.\",\n",
        "        f\"Rewrite this with low {DIMENSION}: I'm happy to help!\",\n",
        "    ]\n",
        "else:\n",
        "    test_prompts = [\n",
        "        f\"Explain a basic concept in {DIMENSION}.\",\n",
        "        f\"What should a beginner know about {DIMENSION}?\",\n",
        "    ]\n",
        "\n",
        "print(\"\\nResults:\\n\")\n",
        "all_ok = True\n",
        "for prompt in test_prompts:\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Response: {response}\\n\")\n",
        "    \n",
        "    # Check for garbled output\n",
        "    if \"Question:\" in response or \"}}\" in response or \"QR\" in response:\n",
        "        print(\"âš ï¸  WARNING: Output looks garbled!\")\n",
        "        all_ok = False\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\nâœ… Model looks good!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Model may need retraining with even lower LR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12: Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()\n",
        "\n",
        "# Check progress\n",
        "import os\n",
        "models_dir = \"/content/drive/MyDrive/Lily/models\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING PROGRESS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_dims = PERSONALITY_DIMS + KNOWLEDGE_DIMS\n",
        "completed = 0\n",
        "\n",
        "for dim in all_dims:\n",
        "    prefix = \"mistral\" if dim in PERSONALITY_DIMS else \"phi3\"\n",
        "    path = f\"{models_dir}/{prefix}_{dim}\"\n",
        "    exists = os.path.exists(path)\n",
        "    icon = \"âœ…\" if exists else \"â¬œ\"\n",
        "    print(f\"  {icon} {dim} ({prefix})\")\n",
        "    if exists:\n",
        "        completed += 1\n",
        "\n",
        "print(f\"\\nProgress: {completed}/8\")\n",
        "\n",
        "if completed < 8:\n",
        "    remaining = [d for d in all_dims if not os.path.exists(\n",
        "        f\"{models_dir}/{'mistral' if d in PERSONALITY_DIMS else 'phi3'}_{d}\"\n",
        "    )]\n",
        "    print(f\"\\nðŸ‘‰ Next: Change DIMENSION = '{remaining[0]}' and run again\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
