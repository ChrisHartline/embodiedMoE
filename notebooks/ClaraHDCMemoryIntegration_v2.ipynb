{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLARA + HDC MEMORY INTEGRATION (v2)\n",
        "\n",
        "This notebook integrates Hyperdimensional Computing (HDC) memory into Clara's dual-brain architecture.\n",
        "\n",
        "**v2 Changes:**\n",
        "- Fixed memory storage: stores actual conversational content, not meta-summaries\n",
        "- Lowered recall threshold (0.25 ‚Üí 0.15) for better conversational follow-ups\n",
        "- Added entity extraction for key conversational elements\n",
        "- Stores both user queries AND Clara's responses\n",
        "- Better debug output to track memory usage\n",
        "\n",
        "**Prerequisites:**\n",
        "- Trained models in Google Drive (`/Lily/models/`)\n",
        "- `clara-knowledge` (Phi-3 merged)\n",
        "- `mistral_warmth`, `mistral_playful`, `mistral_encouragement` adapters\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "User Query ‚Üí HDC Memory Context ‚Üí Semantic Router ‚Üí Brain Selection ‚Üí Response\n",
        "                    ‚Üë                                                    ‚îÇ\n",
        "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Store Interaction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SETUP\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q peft sentence-transformers\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SETUP COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Check Existing Models (Safety Check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAFETY CHECK - Verify existing models without overwriting\n",
        "# ============================================================\n",
        "\n",
        "MODELS_DIR = \"/content/drive/MyDrive/Lily/models\"\n",
        "MEMORY_DIR = \"/content/drive/MyDrive/Lily/memory\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL VERIFICATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Required models\n",
        "required_models = {\n",
        "    \"clara-knowledge\": \"Phi-3 merged knowledge brain\",\n",
        "    \"mistral_warmth\": \"Personality adapter (warmth)\",\n",
        "    \"mistral_playful\": \"Personality adapter (playful)\", \n",
        "    \"mistral_encouragement\": \"Personality adapter (encouragement)\",\n",
        "}\n",
        "\n",
        "all_present = True\n",
        "model_status = {}\n",
        "\n",
        "print(\"\\nüìÅ Checking models in:\", MODELS_DIR)\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for model_name, description in required_models.items():\n",
        "    path = os.path.join(MODELS_DIR, model_name)\n",
        "    \n",
        "    # Check for key files that indicate a valid model\n",
        "    if model_name == \"clara-knowledge\":\n",
        "        # Merged model should have config.json\n",
        "        check_file = os.path.join(path, \"config.json\")\n",
        "    else:\n",
        "        # LoRA adapters have adapter_config.json\n",
        "        check_file = os.path.join(path, \"adapter_config.json\")\n",
        "    \n",
        "    exists = os.path.exists(check_file)\n",
        "    model_status[model_name] = exists\n",
        "    \n",
        "    if exists:\n",
        "        # Get size\n",
        "        total_size = sum(\n",
        "            os.path.getsize(os.path.join(path, f))\n",
        "            for f in os.listdir(path)\n",
        "            if os.path.isfile(os.path.join(path, f))\n",
        "        ) / 1e9\n",
        "        print(f\"  ‚úÖ {model_name:<25} ({total_size:.2f} GB)\")\n",
        "        print(f\"      ‚îî‚îÄ {description}\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå {model_name:<25} MISSING!\")\n",
        "        print(f\"      ‚îî‚îÄ {description}\")\n",
        "        all_present = False\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if all_present:\n",
        "    print(\"\\n‚úÖ All required models found!\")\n",
        "    print(\"   Models will NOT be overwritten.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some models are missing!\")\n",
        "    print(\"   Please run your training notebook first.\")\n",
        "    print(\"   This notebook requires pre-trained models.\")\n",
        "\n",
        "# Check/create memory directory\n",
        "print(\"\\nüìÅ Memory directory:\", MEMORY_DIR)\n",
        "if not os.path.exists(MEMORY_DIR):\n",
        "    os.makedirs(MEMORY_DIR)\n",
        "    print(\"   Created new memory directory\")\n",
        "else:\n",
        "    # Check for existing memory files\n",
        "    memory_files = [f for f in os.listdir(MEMORY_DIR) if f.endswith('.json')]\n",
        "    if memory_files:\n",
        "        print(f\"   Found {len(memory_files)} existing memory file(s):\")\n",
        "        for mf in memory_files:\n",
        "            size = os.path.getsize(os.path.join(MEMORY_DIR, mf)) / 1024\n",
        "            print(f\"      ‚îî‚îÄ {mf} ({size:.1f} KB)\")\n",
        "    else:\n",
        "        print(\"   No existing memory files (fresh start)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Load Semantic Router (Embedder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SEMANTIC ROUTER - Load embedder and domain descriptions\n",
        "# ============================================================\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING SEMANTIC ROUTER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1. Loading embedding model...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"   ‚úÖ all-MiniLM-L6-v2 loaded\")\n",
        "\n",
        "# Domain descriptions (refined from your notebook)\n",
        "DOMAIN_DESCRIPTIONS = {\n",
        "    \"medical\": \"\"\"\n",
        "        symptoms diagnosis treatment disease illness pain fever infection\n",
        "        headache nauseous dizzy blood pressure heart lungs brain body\n",
        "        doctor hospital medicine medication prescription surgery vaccine\n",
        "        virus bacteria immune system allergies chronic acute patient health\n",
        "        tired fatigue exhausted sleep insomnia rash swollen sore throat\n",
        "        cough breathing chest stomach ache injury wound bleeding\n",
        "    \"\"\",\n",
        "    \n",
        "    \"coding\": \"\"\"\n",
        "        programming code software python javascript java function method\n",
        "        variable array list dictionary tuple loop error exception bug debug\n",
        "        API database SQL server backend frontend algorithm data structure\n",
        "        class object inheritance compile runtime syntax IndexError TypeError\n",
        "        iterate parse return import library framework git repository\n",
        "        crash deploy package module script terminal command line\n",
        "        Flask Django React Node npm pip install developer\n",
        "    \"\"\",\n",
        "    \n",
        "    \"teaching\": \"\"\"\n",
        "        explain how does work basics fundamentals introduction tutorial\n",
        "        step by step concept theory lesson learn teach education student\n",
        "        example analogy walk through ELI5 for dummies guide overview\n",
        "        what is the difference between simple explanation textbook\n",
        "        homework assignment class course study\n",
        "    \"\"\",\n",
        "    \n",
        "    \"quantum\": \"\"\"\n",
        "        quantum physics qubit superposition entanglement wave function\n",
        "        particle measurement collapse observer Schrodinger Heisenberg\n",
        "        quantum computer quantum gate Hadamard CNOT quantum circuit\n",
        "        coherence decoherence probability amplitude interference\n",
        "        quantum mechanics quantum state Planck photon electron spin\n",
        "        two places at once two states uncertainty principle\n",
        "        parallel universes both alive and dead particle wave duality\n",
        "    \"\"\",\n",
        "    \n",
        "    \"personality\": \"\"\"\n",
        "        feeling emotion mood happy sad angry anxious worried stressed\n",
        "        excited nervous scared lonely depressed overwhelmed frustrated\n",
        "        relationship friend family love support talk vent chat\n",
        "        my day rough tough great amazing terrible celebrate\n",
        "        broke up promotion new job interview date\n",
        "        grateful appreciate thankful thanks thank you\n",
        "        hey hi hello how are you good morning good night\n",
        "        nobody understands me moving away miss you\n",
        "        sorry to hear congratulations best friend\n",
        "        take care see you later nice to meet you\n",
        "        coffee tea drink offer gave give\n",
        "    \"\"\",\n",
        "}\n",
        "\n",
        "print(\"\\n2. Computing domain embeddings...\")\n",
        "domain_embeddings = {}\n",
        "for domain, description in DOMAIN_DESCRIPTIONS.items():\n",
        "    clean_desc = \" \".join(description.split())\n",
        "    domain_embeddings[domain] = embedder.encode(clean_desc)\n",
        "    print(f\"   ‚úÖ {domain}\")\n",
        "\n",
        "print(\"\\n‚úÖ Semantic router ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: HDC Memory System (v2 - Fixed Recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HDC MEMORY SYSTEM (v2 - Fixed Recall)\n",
        "# ============================================================\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple, Dict, Optional, Set\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"HDC MEMORY SYSTEM v2\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "@dataclass\n",
        "class Memory:\n",
        "    \"\"\"Single memory unit with richer metadata\"\"\"\n",
        "    text: str\n",
        "    timestamp: float\n",
        "    memory_type: str  # 'user_input', 'clara_response', 'preference', 'fact'\n",
        "    importance: float = 0.5\n",
        "    domain: str = \"general\"\n",
        "    entities: List[str] = field(default_factory=list)  # Extracted entities\n",
        "    turn_id: int = 0  # Conversation turn number\n",
        "\n",
        "\n",
        "class ClaraHDCMemory:\n",
        "    \"\"\"\n",
        "    Hyperdimensional Computing Memory System for Clara (v2)\n",
        "    \n",
        "    v2 Improvements:\n",
        "    - Stores actual conversational content (not meta-summaries)\n",
        "    - Entity extraction for key nouns/concepts\n",
        "    - Lower recall threshold (0.15) for conversational follow-ups\n",
        "    - Turn tracking for conversation coherence\n",
        "    - Debug mode for troubleshooting\n",
        "    \n",
        "    HDC Operations:\n",
        "    - Bind (‚äó): Element-wise multiplication - creates associations\n",
        "    - Bundle (+): Element-wise addition + sign - superposition\n",
        "    - Similarity: Cosine similarity for retrieval\n",
        "    \"\"\"\n",
        "    \n",
        "    # Recall threshold - LOWERED from 0.25 to 0.15 for better conversational recall\n",
        "    RECALL_THRESHOLD = 0.15\n",
        "    \n",
        "    # Memory boost threshold for routing\n",
        "    MEMORY_BOOST_THRESHOLD = 0.20\n",
        "    \n",
        "    def __init__(self, embedder, dim: int = 10000, seed: int = 42, debug: bool = False):\n",
        "        self.dim = dim\n",
        "        self.embedder = embedder\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.debug = debug\n",
        "        self.current_turn = 0\n",
        "        \n",
        "        # Random projection matrix: 384-dim ‚Üí 10K-dim\n",
        "        print(\"   Initializing projection matrix...\")\n",
        "        self.projection = self.rng.randn(384, dim).astype(np.float32)\n",
        "        self.projection /= np.linalg.norm(self.projection, axis=1, keepdims=True)\n",
        "        \n",
        "        # Memory stores\n",
        "        self.memories: List[Tuple[np.ndarray, Memory]] = []\n",
        "        self.memory_bundle = np.zeros(dim, dtype=np.float32)\n",
        "        \n",
        "        # Symbol library for structured binding\n",
        "        print(\"   Building symbol library...\")\n",
        "        self.symbols: Dict[str, np.ndarray] = {}\n",
        "        self._init_symbols()\n",
        "        \n",
        "        # Entity index for fast lookup\n",
        "        self.entity_index: Dict[str, List[int]] = {}  # entity -> memory indices\n",
        "        \n",
        "        # Personality vectors\n",
        "        print(\"   Encoding personality vectors...\")\n",
        "        self.personality = self._init_personality()\n",
        "        \n",
        "        print(f\"   ‚úÖ HDC Memory v2 initialized (dim={dim})\")\n",
        "        print(f\"   üìä Recall threshold: {self.RECALL_THRESHOLD}\")\n",
        "    \n",
        "    def _init_symbols(self):\n",
        "        \"\"\"Create base symbol vocabulary\"\"\"\n",
        "        base_symbols = [\n",
        "            # Roles\n",
        "            \"ROLE_USER\", \"ROLE_CLARA\", \"ROLE_TOPIC\", \"ROLE_OUTCOME\", \"ROLE_ENTITY\",\n",
        "            # Actions  \n",
        "            \"ASKED\", \"ANSWERED\", \"OFFERED\", \"RECEIVED\", \"DISCUSSED\",\n",
        "            # Domains\n",
        "            \"MEDICAL\", \"CODING\", \"TEACHING\", \"QUANTUM\", \"PERSONALITY\",\n",
        "            # Common conversational entities\n",
        "            \"COFFEE\", \"TEA\", \"FOOD\", \"DRINK\", \"HELP\", \"THANKS\",\n",
        "            # Outcomes\n",
        "            \"HELPFUL\", \"CONFUSED\", \"SATISFIED\", \"FRUSTRATED\",\n",
        "            # Time markers\n",
        "            \"RECENT\", \"TODAY\", \"THIS_SESSION\", \"EARLIER\"\n",
        "        ]\n",
        "        for s in base_symbols:\n",
        "            self.symbols[s] = self._random_hv()\n",
        "    \n",
        "    def _init_personality(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Encode Clara's personality traits as hypervectors\"\"\"\n",
        "        traits = {\n",
        "            'warmth': 0.85,\n",
        "            'curiosity': 0.75,\n",
        "            'patience': 0.90,\n",
        "            'encouragement': 0.80,\n",
        "        }\n",
        "        personality = {}\n",
        "        for trait, strength in traits.items():\n",
        "            base_hv = self._random_hv()\n",
        "            personality[trait] = (base_hv * strength).astype(np.float32)\n",
        "        \n",
        "        all_traits = list(personality.values())\n",
        "        personality['composite'] = np.sign(\n",
        "            np.sum(all_traits, axis=0)\n",
        "        ).astype(np.float32)\n",
        "        \n",
        "        return personality\n",
        "    \n",
        "    def _random_hv(self) -> np.ndarray:\n",
        "        \"\"\"Generate random bipolar hypervector {-1, +1}\"\"\"\n",
        "        return self.rng.choice([-1, 1], size=self.dim).astype(np.float32)\n",
        "    \n",
        "    def _text_to_hv(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Convert text to hypervector using existing embedder\"\"\"\n",
        "        embedding = self.embedder.encode(text)\n",
        "        hv = embedding @ self.projection\n",
        "        return np.sign(hv).astype(np.float32)\n",
        "    \n",
        "    def _get_symbol(self, name: str) -> np.ndarray:\n",
        "        \"\"\"Get or create symbol\"\"\"\n",
        "        name_upper = name.upper().replace(\" \", \"_\")\n",
        "        if name_upper not in self.symbols:\n",
        "            self.symbols[name_upper] = self._random_hv()\n",
        "        return self.symbols[name_upper]\n",
        "    \n",
        "    def _extract_entities(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract key entities from text for indexing\n",
        "        Simple extraction - looks for nouns and key terms\n",
        "        \"\"\"\n",
        "        # Common conversational entities to look for\n",
        "        entity_patterns = [\n",
        "            r'\\bcoffee\\b', r'\\btea\\b', r'\\bwater\\b', r'\\bdrink\\b',\n",
        "            r'\\bfood\\b', r'\\blunch\\b', r'\\bdinner\\b', r'\\bbreakfast\\b',\n",
        "            r'\\bproject\\b', r'\\bwork\\b', r'\\bmeeting\\b', r'\\btask\\b',\n",
        "            r'\\bDocker\\b', r'\\bPython\\b', r'\\bcode\\b', r'\\bnotebook\\b',\n",
        "            r'\\bhelp\\b', r'\\bthanks\\b', r'\\bplease\\b',\n",
        "        ]\n",
        "        \n",
        "        entities = []\n",
        "        text_lower = text.lower()\n",
        "        \n",
        "        for pattern in entity_patterns:\n",
        "            if re.search(pattern, text_lower):\n",
        "                # Extract the matched word\n",
        "                match = re.search(pattern, text_lower)\n",
        "                if match:\n",
        "                    entities.append(match.group().upper())\n",
        "        \n",
        "        return list(set(entities))  # Dedupe\n",
        "    \n",
        "    # === HDC Operations ===\n",
        "    \n",
        "    def bind(self, hv1: np.ndarray, hv2: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Bind two hypervectors (‚äó) - creates association\"\"\"\n",
        "        return hv1 * hv2\n",
        "    \n",
        "    def bundle(self, hvs: List[np.ndarray]) -> np.ndarray:\n",
        "        \"\"\"Bundle hypervectors (+) - superposition\"\"\"\n",
        "        if not hvs:\n",
        "            return np.zeros(self.dim, dtype=np.float32)\n",
        "        return np.sign(np.sum(hvs, axis=0)).astype(np.float32)\n",
        "    \n",
        "    def similarity(self, hv1: np.ndarray, hv2: np.ndarray) -> float:\n",
        "        \"\"\"Cosine similarity between hypervectors\"\"\"\n",
        "        n1, n2 = np.linalg.norm(hv1), np.linalg.norm(hv2)\n",
        "        if n1 == 0 or n2 == 0:\n",
        "            return 0.0\n",
        "        return float(np.dot(hv1, hv2) / (n1 * n2))\n",
        "    \n",
        "    # === Memory Operations ===\n",
        "    \n",
        "    def store(self, text: str, memory_type: str = \"user_input\",\n",
        "              importance: float = 0.5, domain: str = \"general\",\n",
        "              increment_turn: bool = False, **bindings) -> None:\n",
        "        \"\"\"\n",
        "        Store a memory with entity extraction and optional bindings\n",
        "        \n",
        "        Args:\n",
        "            text: The actual conversational content (NOT a summary!)\n",
        "            memory_type: 'user_input', 'clara_response', 'preference', 'fact'\n",
        "            importance: 0.0-1.0 importance score\n",
        "            domain: 'medical', 'coding', 'teaching', 'quantum', 'personality'\n",
        "            increment_turn: Whether this starts a new conversation turn\n",
        "            **bindings: Structured role-filler pairs\n",
        "        \"\"\"\n",
        "        if increment_turn:\n",
        "            self.current_turn += 1\n",
        "        \n",
        "        # Extract entities from text\n",
        "        entities = self._extract_entities(text)\n",
        "        \n",
        "        # Create semantic hypervector from ACTUAL text\n",
        "        text_hv = self._text_to_hv(text)\n",
        "        \n",
        "        # Add domain binding\n",
        "        domain_hv = self._get_symbol(domain.upper())\n",
        "        text_hv = self.bind(text_hv, domain_hv)\n",
        "        \n",
        "        # Add entity bindings\n",
        "        if entities:\n",
        "            entity_hvs = []\n",
        "            for entity in entities:\n",
        "                entity_hv = self._get_symbol(entity)\n",
        "                role_hv = self._get_symbol(\"ROLE_ENTITY\")\n",
        "                entity_hvs.append(self.bind(role_hv, entity_hv))\n",
        "            if entity_hvs:\n",
        "                entity_bundle = self.bundle(entity_hvs)\n",
        "                text_hv = self.bundle([text_hv, entity_bundle])\n",
        "        \n",
        "        # Add structural bindings if provided\n",
        "        if bindings:\n",
        "            bound_parts = []\n",
        "            for role, filler in bindings.items():\n",
        "                role_hv = self._get_symbol(f\"ROLE_{role.upper()}\")\n",
        "                filler_hv = self._get_symbol(str(filler).upper())\n",
        "                bound_parts.append(self.bind(role_hv, filler_hv))\n",
        "            if bound_parts:\n",
        "                structure_hv = self.bundle(bound_parts)\n",
        "                text_hv = self.bundle([text_hv, structure_hv])\n",
        "        \n",
        "        memory = Memory(\n",
        "            text=text,\n",
        "            timestamp=time.time(),\n",
        "            memory_type=memory_type,\n",
        "            importance=importance,\n",
        "            domain=domain,\n",
        "            entities=entities,\n",
        "            turn_id=self.current_turn\n",
        "        )\n",
        "        \n",
        "        memory_idx = len(self.memories)\n",
        "        self.memories.append((text_hv, memory))\n",
        "        \n",
        "        # Update entity index\n",
        "        for entity in entities:\n",
        "            if entity not in self.entity_index:\n",
        "                self.entity_index[entity] = []\n",
        "            self.entity_index[entity].append(memory_idx)\n",
        "        \n",
        "        # Update bundled representation\n",
        "        self.memory_bundle = self.bundle([self.memory_bundle, text_hv])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(f\"      [MEM] Stored: '{text[:50]}...'\")\n",
        "            print(f\"            Entities: {entities}\")\n",
        "            print(f\"            Turn: {self.current_turn}, Importance: {importance:.2f}\")\n",
        "    \n",
        "    def recall(self, query: str, top_k: int = 5,\n",
        "               domain_filter: Optional[str] = None,\n",
        "               include_entities: bool = True) -> List[Tuple[Memory, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve memories similar to query\n",
        "        \n",
        "        v2: Also boosts memories that share entities with the query\n",
        "        \"\"\"\n",
        "        if not self.memories:\n",
        "            return []\n",
        "        \n",
        "        query_hv = self._text_to_hv(query)\n",
        "        query_entities = self._extract_entities(query) if include_entities else []\n",
        "        \n",
        "        if self.debug:\n",
        "            print(f\"      [RECALL] Query: '{query[:40]}...'\")\n",
        "            print(f\"               Entities: {query_entities}\")\n",
        "        \n",
        "        results = []\n",
        "        for idx, (hv, memory) in enumerate(self.memories):\n",
        "            # Apply domain filter if specified\n",
        "            if domain_filter and memory.domain != domain_filter:\n",
        "                continue\n",
        "            \n",
        "            # Base similarity\n",
        "            sim = self.similarity(query_hv, hv)\n",
        "            \n",
        "            # Entity overlap boost\n",
        "            entity_boost = 0.0\n",
        "            if query_entities and memory.entities:\n",
        "                overlap = set(query_entities) & set(memory.entities)\n",
        "                if overlap:\n",
        "                    entity_boost = 0.15 * len(overlap)  # Boost per shared entity\n",
        "                    if self.debug:\n",
        "                        print(f\"               Entity match: {overlap} (+{entity_boost:.2f})\")\n",
        "            \n",
        "            # Recency boost (stronger for recent turns)\n",
        "            turn_diff = self.current_turn - memory.turn_id\n",
        "            recency = 1.0 / (1.0 + turn_diff * 0.1)  # Slower decay\n",
        "            \n",
        "            # Time-based recency (for cross-session)\n",
        "            age_hours = (time.time() - memory.timestamp) / 3600\n",
        "            time_recency = 1.0 / (1.0 + age_hours / 24)\n",
        "            \n",
        "            # Combined score\n",
        "            weighted = (\n",
        "                sim + entity_boost\n",
        "            ) * (0.6 + 0.2 * memory.importance) * (0.7 + 0.15 * recency + 0.15 * time_recency)\n",
        "            \n",
        "            results.append((memory, weighted))\n",
        "            \n",
        "            if self.debug and sim > 0.1:\n",
        "                print(f\"               [{idx}] sim={sim:.3f} final={weighted:.3f} '{memory.text[:30]}...'\")\n",
        "        \n",
        "        results.sort(key=lambda x: x[1], reverse=True)\n",
        "        return results[:top_k]\n",
        "    \n",
        "    def recall_by_entity(self, entity: str) -> List[Memory]:\n",
        "        \"\"\"Fast lookup by entity name\"\"\"\n",
        "        entity_upper = entity.upper()\n",
        "        if entity_upper not in self.entity_index:\n",
        "            return []\n",
        "        \n",
        "        indices = self.entity_index[entity_upper]\n",
        "        return [self.memories[i][1] for i in indices]\n",
        "    \n",
        "    def get_context_for_routing(self, query: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get memory-augmented context for routing decisions\n",
        "        \"\"\"\n",
        "        query_hv = self._text_to_hv(query)\n",
        "        \n",
        "        relevant = self.recall(query, top_k=3)\n",
        "        if relevant:\n",
        "            memory_hvs = [self._text_to_hv(m.text) for m, _ in relevant]\n",
        "            memory_context = self.bundle(memory_hvs)\n",
        "        else:\n",
        "            memory_context = np.zeros(self.dim, dtype=np.float32)\n",
        "        \n",
        "        routing_hv = self.bundle([\n",
        "            query_hv,\n",
        "            memory_context * 0.3,\n",
        "            self.personality['composite'] * 0.2\n",
        "        ])\n",
        "        \n",
        "        return routing_hv\n",
        "    \n",
        "    def get_context_string(self, query: str, max_memories: int = 3) -> str:\n",
        "        \"\"\"\n",
        "        Get memory context as string for prompt injection\n",
        "        \n",
        "        v2: Lower threshold, better formatting\n",
        "        \"\"\"\n",
        "        relevant = self.recall(query, top_k=max_memories)\n",
        "        if not relevant:\n",
        "            return \"\"\n",
        "        \n",
        "        # Use class threshold constant\n",
        "        good_memories = [(m, s) for m, s in relevant if s > self.RECALL_THRESHOLD]\n",
        "        \n",
        "        if self.debug:\n",
        "            print(f\"      [CONTEXT] {len(relevant)} retrieved, {len(good_memories)} above threshold ({self.RECALL_THRESHOLD})\")\n",
        "        \n",
        "        if not good_memories:\n",
        "            return \"\"\n",
        "        \n",
        "        context_parts = [\"[Conversation context:\"]\n",
        "        for mem, score in good_memories:\n",
        "            # Format based on memory type\n",
        "            if mem.memory_type == \"user_input\":\n",
        "                context_parts.append(f\"- User said: {mem.text[:100]}\")\n",
        "            elif mem.memory_type == \"clara_response\":\n",
        "                context_parts.append(f\"- Clara said: {mem.text[:100]}\")\n",
        "            else:\n",
        "                context_parts.append(f\"- {mem.text[:100]}\")\n",
        "        context_parts.append(\"]\")\n",
        "        \n",
        "        return \"\\n\".join(context_parts)\n",
        "    \n",
        "    def get_recent_context(self, n_turns: int = 2) -> str:\n",
        "        \"\"\"\n",
        "        Get context from most recent N conversation turns\n",
        "        (Fallback for when semantic search doesn't find matches)\n",
        "        \"\"\"\n",
        "        if not self.memories:\n",
        "            return \"\"\n",
        "        \n",
        "        min_turn = max(0, self.current_turn - n_turns)\n",
        "        recent = [\n",
        "            m for _, m in self.memories \n",
        "            if m.turn_id >= min_turn\n",
        "        ]\n",
        "        \n",
        "        if not recent:\n",
        "            return \"\"\n",
        "        \n",
        "        context_parts = [\"[Recent conversation:\"]\n",
        "        for mem in recent[-6:]:  # Last 6 memories max\n",
        "            if mem.memory_type == \"user_input\":\n",
        "                context_parts.append(f\"- User: {mem.text[:80]}\")\n",
        "            elif mem.memory_type == \"clara_response\":\n",
        "                context_parts.append(f\"- Clara: {mem.text[:80]}\")\n",
        "        context_parts.append(\"]\")\n",
        "        \n",
        "        return \"\\n\".join(context_parts)\n",
        "    \n",
        "    def stats(self) -> Dict:\n",
        "        \"\"\"Get memory statistics\"\"\"\n",
        "        domain_counts = {}\n",
        "        type_counts = {}\n",
        "        for _, m in self.memories:\n",
        "            domain_counts[m.domain] = domain_counts.get(m.domain, 0) + 1\n",
        "            type_counts[m.memory_type] = type_counts.get(m.memory_type, 0) + 1\n",
        "        \n",
        "        return {\n",
        "            \"total_memories\": len(self.memories),\n",
        "            \"by_domain\": domain_counts,\n",
        "            \"by_type\": type_counts,\n",
        "            \"symbols\": len(self.symbols),\n",
        "            \"entities_tracked\": len(self.entity_index),\n",
        "            \"current_turn\": self.current_turn,\n",
        "            \"size_kb\": self.size_bytes() / 1024\n",
        "        }\n",
        "    \n",
        "    def size_bytes(self) -> int:\n",
        "        \"\"\"Memory footprint (excluding embedder)\"\"\"\n",
        "        bundle_size = self.memory_bundle.nbytes\n",
        "        memories_size = sum(hv.nbytes for hv, _ in self.memories)\n",
        "        symbols_size = sum(hv.nbytes for hv in self.symbols.values())\n",
        "        personality_size = sum(hv.nbytes for hv in self.personality.values())\n",
        "        projection_size = self.projection.nbytes\n",
        "        return bundle_size + memories_size + symbols_size + personality_size + projection_size\n",
        "    \n",
        "    # === Persistence ===\n",
        "    \n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save memory state to file\"\"\"\n",
        "        data = {\n",
        "            'version': '2.0',\n",
        "            'dim': self.dim,\n",
        "            'current_turn': self.current_turn,\n",
        "            'recall_threshold': self.RECALL_THRESHOLD,\n",
        "            'memories': [\n",
        "                {\n",
        "                    'hv': hv.tolist(),\n",
        "                    'text': m.text,\n",
        "                    'timestamp': m.timestamp,\n",
        "                    'memory_type': m.memory_type,\n",
        "                    'importance': m.importance,\n",
        "                    'domain': m.domain,\n",
        "                    'entities': m.entities,\n",
        "                    'turn_id': m.turn_id\n",
        "                }\n",
        "                for hv, m in self.memories\n",
        "            ],\n",
        "            'symbols': {k: v.tolist() for k, v in self.symbols.items()},\n",
        "            'entity_index': self.entity_index,\n",
        "            'memory_bundle': self.memory_bundle.tolist(),\n",
        "            'projection': self.projection.tolist()\n",
        "        }\n",
        "        \n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(data, f)\n",
        "        \n",
        "        print(f\"‚úÖ Saved {len(self.memories)} memories to {path}\")\n",
        "        print(f\"   Size: {os.path.getsize(path) / 1024:.1f} KB\")\n",
        "    \n",
        "    def load(self, path: str) -> bool:\n",
        "        \"\"\"Load memory state from file\"\"\"\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"‚ö†Ô∏è No memory file found at {path}\")\n",
        "            return False\n",
        "        \n",
        "        with open(path) as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # Check version\n",
        "        version = data.get('version', '1.0')\n",
        "        print(f\"   Loading memory file version {version}\")\n",
        "        \n",
        "        if data.get('dim') != self.dim:\n",
        "            print(f\"‚ö†Ô∏è Dimension mismatch: file has {data.get('dim')}, expected {self.dim}\")\n",
        "            return False\n",
        "        \n",
        "        # Load memories\n",
        "        self.memories = []\n",
        "        for m in data['memories']:\n",
        "            memory = Memory(\n",
        "                text=m['text'],\n",
        "                timestamp=m['timestamp'],\n",
        "                memory_type=m.get('memory_type', 'interaction'),\n",
        "                importance=m['importance'],\n",
        "                domain=m.get('domain', 'general'),\n",
        "                entities=m.get('entities', []),\n",
        "                turn_id=m.get('turn_id', 0)\n",
        "            )\n",
        "            hv = np.array(m['hv'], dtype=np.float32)\n",
        "            self.memories.append((hv, memory))\n",
        "        \n",
        "        # Load other state\n",
        "        self.symbols = {k: np.array(v, dtype=np.float32) for k, v in data['symbols'].items()}\n",
        "        self.entity_index = data.get('entity_index', {})\n",
        "        self.memory_bundle = np.array(data['memory_bundle'], dtype=np.float32)\n",
        "        self.current_turn = data.get('current_turn', 0)\n",
        "        \n",
        "        if 'projection' in data:\n",
        "            self.projection = np.array(data['projection'], dtype=np.float32)\n",
        "        \n",
        "        print(f\"‚úÖ Loaded {len(self.memories)} memories from {path}\")\n",
        "        return True\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Initialize HDC Memory\n",
        "# ============================================================\n",
        "print(\"\\nInitializing Clara's HDC Memory v2...\")\n",
        "\n",
        "# Set debug=True to see memory operations\n",
        "DEBUG_MEMORY = True\n",
        "\n",
        "clara_memory = ClaraHDCMemory(embedder=embedder, dim=10000, debug=DEBUG_MEMORY)\n",
        "\n",
        "# Try to load existing memory\n",
        "MEMORY_FILE = os.path.join(MEMORY_DIR, \"clara_memory_v2.json\")\n",
        "if os.path.exists(MEMORY_FILE):\n",
        "    print(f\"\\nFound existing memory file, loading...\")\n",
        "    clara_memory.load(MEMORY_FILE)\n",
        "else:\n",
        "    print(f\"\\nNo existing memory file - starting fresh\")\n",
        "\n",
        "print(f\"\\nüìä Memory Stats:\")\n",
        "stats = clara_memory.stats()\n",
        "print(f\"   Total memories: {stats['total_memories']}\")\n",
        "print(f\"   Current turn: {stats['current_turn']}\")\n",
        "print(f\"   Entities tracked: {stats['entities_tracked']}\")\n",
        "print(f\"   Symbols: {stats['symbols']}\")\n",
        "print(f\"   Size: {stats['size_kb']:.1f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Smart Router with Memory (v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SMART ROUTER v2 (with improved memory integration)\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SMART ROUTER v2\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def smart_route(query: str, use_memory: bool = True, \n",
        "                threshold: float = 0.20) -> tuple:\n",
        "    \"\"\"\n",
        "    Route query using semantic similarity + memory context\n",
        "    \n",
        "    v2: Lower memory boost threshold (0.30 ‚Üí 0.20)\n",
        "    \"\"\"\n",
        "    query_embedding = embedder.encode(query)\n",
        "    \n",
        "    similarities = {}\n",
        "    for domain, domain_emb in domain_embeddings.items():\n",
        "        similarity = np.dot(query_embedding, domain_emb) / (\n",
        "            np.linalg.norm(query_embedding) * np.linalg.norm(domain_emb)\n",
        "        )\n",
        "        similarities[domain] = similarity\n",
        "    \n",
        "    # Memory-based boosting with LOWER threshold\n",
        "    memory_context_used = False\n",
        "    if use_memory and len(clara_memory.memories) > 0:\n",
        "        relevant = clara_memory.recall(query, top_k=2)\n",
        "        if relevant:\n",
        "            for mem, score in relevant:\n",
        "                # v2: Lowered from 0.30 to 0.20\n",
        "                if score > clara_memory.MEMORY_BOOST_THRESHOLD and mem.domain in similarities:\n",
        "                    similarities[mem.domain] += 0.05\n",
        "                    memory_context_used = True\n",
        "    \n",
        "    sorted_domains = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "    best_domain, best_conf = sorted_domains[0]\n",
        "    second_domain, second_conf = sorted_domains[1]\n",
        "    \n",
        "    # If close, prefer personality (warmth)\n",
        "    if best_conf - second_conf < 0.05:\n",
        "        if second_domain == \"personality\":\n",
        "            best_domain = \"personality\"\n",
        "            best_conf = second_conf\n",
        "    \n",
        "    # Low confidence fallback\n",
        "    if best_conf < threshold and best_domain != \"personality\":\n",
        "        if similarities[\"personality\"] > 0.15:\n",
        "            best_domain = \"personality\"\n",
        "            best_conf = similarities[\"personality\"]\n",
        "    \n",
        "    if best_domain == \"personality\":\n",
        "        brain = \"personality\"\n",
        "        domain = \"warmth\"\n",
        "    else:\n",
        "        brain = \"knowledge\"\n",
        "        domain = best_domain\n",
        "    \n",
        "    return brain, domain, best_conf, memory_context_used\n",
        "\n",
        "\n",
        "def clean_response(response: str) -> str:\n",
        "    \"\"\"Clean up response artifacts\"\"\"\n",
        "    stop_markers = [\"### Instruction:\", \"Instruction:\", \"\\n\\n\\n\", \"User:\", \"[Conversation context:\"]\n",
        "    for marker in stop_markers:\n",
        "        if marker in response:\n",
        "            response = response.split(marker)[0].strip()\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "print(\"‚úÖ Router v2 functions defined\")\n",
        "\n",
        "# Quick test\n",
        "print(\"\\nüìã Router test:\")\n",
        "test_queries = [\n",
        "    \"How do I read a CSV in Python?\",\n",
        "    \"I'm feeling stressed about work\",\n",
        "    \"What is quantum entanglement?\"\n",
        "]\n",
        "for q in test_queries:\n",
        "    brain, domain, conf, mem_used = smart_route(q)\n",
        "    mem_icon = \"üß†\" if mem_used else \"  \"\n",
        "    print(f\"   {mem_icon} {q[:40]:40} ‚Üí {brain}/{domain} ({conf:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Load Clara's Brains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD CLARA'S DUAL-BRAIN SYSTEM\n",
        "# ============================================================\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING CLARA'S BRAINS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# KNOWLEDGE BRAIN (Phi-3 merged)\n",
        "# ============================================================\n",
        "print(\"\\n1. Loading Knowledge Brain (Phi-3)...\")\n",
        "\n",
        "knowledge_path = os.path.join(MODELS_DIR, \"clara-knowledge\")\n",
        "\n",
        "if not os.path.exists(knowledge_path):\n",
        "    raise FileNotFoundError(f\"Knowledge model not found: {knowledge_path}\")\n",
        "\n",
        "knowledge_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    knowledge_path,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "knowledge_model = AutoModelForCausalLM.from_pretrained(\n",
        "    knowledge_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "knowledge_model.eval()\n",
        "print(\"   ‚úÖ Knowledge brain loaded\")\n",
        "\n",
        "# ============================================================\n",
        "# PERSONALITY BRAIN (Mistral + LoRA adapters)\n",
        "# ============================================================\n",
        "print(\"\\n2. Loading Personality Brain (Mistral + adapters)...\")\n",
        "\n",
        "personality_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        ")\n",
        "\n",
        "personality_base = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "adapters_to_load = [\n",
        "    (\"warmth\", \"mistral_warmth\"),\n",
        "    (\"playful\", \"mistral_playful\"),\n",
        "    (\"encouragement\", \"mistral_encouragement\"),\n",
        "]\n",
        "\n",
        "first_name, first_path = adapters_to_load[0]\n",
        "personality_model = PeftModel.from_pretrained(\n",
        "    personality_base,\n",
        "    os.path.join(MODELS_DIR, first_path),\n",
        "    adapter_name=first_name\n",
        ")\n",
        "\n",
        "for adapter_name, adapter_path in adapters_to_load[1:]:\n",
        "    full_path = os.path.join(MODELS_DIR, adapter_path)\n",
        "    if os.path.exists(full_path):\n",
        "        personality_model.load_adapter(full_path, adapter_name=adapter_name)\n",
        "        print(f\"   ‚úÖ Loaded adapter: {adapter_name}\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Adapter not found: {adapter_path}\")\n",
        "\n",
        "personality_model.set_adapter(\"warmth\")\n",
        "personality_model.eval()\n",
        "print(\"   ‚úÖ Personality brain loaded\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üß† CLARA'S BRAINS ARE READY!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Clara Main Interface (v2 - Fixed Memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CLARA - Main Interface v2 (Fixed Memory Storage & Recall)\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CLARA - MAIN INTERFACE v2\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def clara(query: str, verbose: bool = True, \n",
        "          use_memory: bool = True,\n",
        "          store_interaction: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Clara's main interface - v2 with fixed memory!\n",
        "    \n",
        "    v2 Changes:\n",
        "    - Stores ACTUAL query text (not meta-summary)\n",
        "    - Stores Clara's response too\n",
        "    - Uses recent context as fallback\n",
        "    - Better memory debug output\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Route the query\n",
        "    brain, domain, conf, mem_used = smart_route(query, use_memory=use_memory)\n",
        "    \n",
        "    if verbose:\n",
        "        mem_icon = \"üß†\" if mem_used else \"  \"\n",
        "        print(f\"   {mem_icon} Routing: {brain}/{domain} (conf: {conf:.2f})\")\n",
        "    \n",
        "    # 2. Get memory context for prompt\n",
        "    memory_context = \"\"\n",
        "    if use_memory:\n",
        "        # Try semantic recall first\n",
        "        memory_context = clara_memory.get_context_string(query)\n",
        "        \n",
        "        # If no semantic match, use recent conversation context\n",
        "        if not memory_context and clara_memory.current_turn > 0:\n",
        "            memory_context = clara_memory.get_recent_context(n_turns=2)\n",
        "            if memory_context and verbose:\n",
        "                print(f\"   üìö Using recent context (no semantic match)\")\n",
        "        elif memory_context and verbose:\n",
        "            print(f\"   üìö Memory context included (semantic match)\")\n",
        "    \n",
        "    # 3. Generate response\n",
        "    if brain == \"knowledge\":\n",
        "        if memory_context:\n",
        "            prompt = f\"### Instruction:\\n{memory_context}\\n\\nUser question: {query}\\n\\n### Response:\\n\"\n",
        "        else:\n",
        "            prompt = f\"### Instruction:\\n{query}\\n\\n### Response:\\n\"\n",
        "        \n",
        "        inputs = knowledge_tokenizer(prompt, return_tensors=\"pt\").to(knowledge_model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = knowledge_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=250,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=knowledge_tokenizer.eos_token_id,\n",
        "                use_cache=False\n",
        "            )\n",
        "        \n",
        "        response = knowledge_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if \"### Response:\" in response:\n",
        "            response = response.split(\"### Response:\")[-1].strip()\n",
        "    \n",
        "    else:\n",
        "        personality_model.set_adapter(domain)\n",
        "        \n",
        "        if memory_context:\n",
        "            prompt = f\"### Instruction:\\n{memory_context}\\n\\nUser message: {query}\\n\\n### Response:\\n\"\n",
        "        else:\n",
        "            prompt = f\"### Instruction:\\n{query}\\n\\n### Response:\\n\"\n",
        "        \n",
        "        inputs = personality_tokenizer(prompt, return_tensors=\"pt\").to(personality_model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = personality_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=150,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=personality_tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        response = personality_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if \"### Response:\" in response:\n",
        "            response = response.split(\"### Response:\")[-1].strip()\n",
        "    \n",
        "    response = clean_response(response)\n",
        "    \n",
        "    # 4. Store interaction in memory - v2: ACTUAL CONTENT!\n",
        "    if store_interaction:\n",
        "        effective_domain = domain if brain == \"knowledge\" else \"personality\"\n",
        "        \n",
        "        # Store user's actual query (increment turn)\n",
        "        clara_memory.store(\n",
        "            text=query,  # ACTUAL QUERY, not summary!\n",
        "            memory_type=\"user_input\",\n",
        "            importance=0.5 + (conf * 0.3),\n",
        "            domain=effective_domain,\n",
        "            increment_turn=True\n",
        "        )\n",
        "        \n",
        "        # Store Clara's response (truncated)\n",
        "        clara_memory.store(\n",
        "            text=response[:150],  # Store actual response\n",
        "            memory_type=\"clara_response\",\n",
        "            importance=0.3,\n",
        "            domain=effective_domain\n",
        "        )\n",
        "    \n",
        "    return response\n",
        "\n",
        "\n",
        "def clara_remember(text: str, importance: float = 0.7, \n",
        "                   domain: str = \"general\") -> None:\n",
        "    \"\"\"Explicitly store something in Clara's memory\"\"\"\n",
        "    clara_memory.store(\n",
        "        text=text,\n",
        "        memory_type=\"preference\",\n",
        "        importance=importance,\n",
        "        domain=domain\n",
        "    )\n",
        "    print(f\"‚úÖ Stored: {text[:50]}...\")\n",
        "\n",
        "\n",
        "def clara_recall(query: str, top_k: int = 5) -> None:\n",
        "    \"\"\"Query Clara's memory directly\"\"\"\n",
        "    results = clara_memory.recall(query, top_k=top_k)\n",
        "    \n",
        "    print(f\"\\nüîç Memory search: '{query}'\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    if not results:\n",
        "        print(\"   No memories found\")\n",
        "        return\n",
        "    \n",
        "    for mem, score in results:\n",
        "        entities_str = f\" [{', '.join(mem.entities)}]\" if mem.entities else \"\"\n",
        "        print(f\"   [{score:.3f}] [{mem.memory_type:14}] [{mem.domain:10}]{entities_str}\")\n",
        "        print(f\"           '{mem.text[:60]}...'\")\n",
        "\n",
        "\n",
        "def clara_stats() -> None:\n",
        "    \"\"\"Show Clara's memory statistics\"\"\"\n",
        "    stats = clara_memory.stats()\n",
        "    \n",
        "    print(\"\\nüìä Clara's Memory Stats\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"   Total memories: {stats['total_memories']}\")\n",
        "    print(f\"   Current turn: {stats['current_turn']}\")\n",
        "    print(f\"   Entities tracked: {stats['entities_tracked']}\")\n",
        "    print(f\"   Symbols: {stats['symbols']}\")\n",
        "    print(f\"   Size: {stats['size_kb']:.1f} KB\")\n",
        "    \n",
        "    if stats['by_type']:\n",
        "        print(\"\\n   By type:\")\n",
        "        for mtype, count in sorted(stats['by_type'].items()):\n",
        "            print(f\"      {mtype}: {count}\")\n",
        "    \n",
        "    if stats['by_domain']:\n",
        "        print(\"\\n   By domain:\")\n",
        "        for domain, count in sorted(stats['by_domain'].items()):\n",
        "            print(f\"      {domain}: {count}\")\n",
        "\n",
        "\n",
        "def clara_entities() -> None:\n",
        "    \"\"\"Show tracked entities\"\"\"\n",
        "    print(\"\\nüè∑Ô∏è Tracked Entities\")\n",
        "    print(\"-\" * 60)\n",
        "    for entity, indices in sorted(clara_memory.entity_index.items()):\n",
        "        print(f\"   {entity}: {len(indices)} mention(s)\")\n",
        "\n",
        "\n",
        "def clara_save() -> None:\n",
        "    \"\"\"Save Clara's memory to disk\"\"\"\n",
        "    clara_memory.save(MEMORY_FILE)\n",
        "\n",
        "\n",
        "def clara_debug(on: bool = True) -> None:\n",
        "    \"\"\"Toggle debug mode\"\"\"\n",
        "    clara_memory.debug = on\n",
        "    print(f\"Debug mode: {'ON' if on else 'OFF'}\")\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ Clara interface v2 ready!\")\n",
        "print(\"\\nAvailable functions:\")\n",
        "print(\"   clara(query)           - Talk to Clara\")\n",
        "print(\"   clara_remember(text)   - Store something in memory\")\n",
        "print(\"   clara_recall(query)    - Search Clara's memory\")\n",
        "print(\"   clara_stats()          - Show memory statistics\")\n",
        "print(\"   clara_entities()       - Show tracked entities\")\n",
        "print(\"   clara_save()           - Save memory to disk\")\n",
        "print(\"   clara_debug(on/off)    - Toggle debug output\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Test Memory Recall (Coffee Test!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TEST MEMORY RECALL - The Coffee Test!\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TESTING CLARA's MEMORY - The Coffee Test\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Make sure debug is on for this test\n",
        "clara_debug(True)\n",
        "\n",
        "# Clear any existing memories for clean test\n",
        "clara_memory.memories = []\n",
        "clara_memory.memory_bundle = np.zeros(clara_memory.dim, dtype=np.float32)\n",
        "clara_memory.entity_index = {}\n",
        "clara_memory.current_turn = 0\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Turn 1: Offering coffee\")\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nüë§ User: Hi! Would you like some coffee?\")\n",
        "response = clara(\"Hi! Would you like some coffee?\")\n",
        "print(f\"\\nü§ñ Clara: {response}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Turn 2: Different topic\")\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nüë§ User: What's your favorite thing about helping people?\")\n",
        "response = clara(\"What's your favorite thing about helping people?\")\n",
        "print(f\"\\nü§ñ Clara: {response}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Turn 3: THE COFFEE RECALL TEST\")\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nüë§ User: How's the coffee?\")\n",
        "response = clara(\"How's the coffee?\")\n",
        "print(f\"\\nü§ñ Clara: {response}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Memory Analysis\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "clara_stats()\n",
        "clara_entities()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Direct memory search for 'coffee'\")\n",
        "print(\"-\" * 60)\n",
        "clara_recall(\"coffee\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Interactive Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# INTERACTIVE SESSION\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INTERACTIVE CLARA SESSION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nType your message to Clara.\")\n",
        "print(\"Special commands:\")\n",
        "print(\"   /memory     - Show memory stats\")\n",
        "print(\"   /recall X   - Search memory for X\")\n",
        "print(\"   /entities   - Show tracked entities\")\n",
        "print(\"   /debug      - Toggle debug mode\")\n",
        "print(\"   /save       - Save memory to disk\")\n",
        "print(\"   /clear      - Clear all memories\")\n",
        "print(\"   /quit       - End session\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"\\nüë§ You: \").strip()\n",
        "        \n",
        "        if not user_input:\n",
        "            continue\n",
        "        \n",
        "        # Handle special commands\n",
        "        if user_input.lower() == \"/quit\":\n",
        "            print(\"\\nEnding session. Don't forget to save memory!\")\n",
        "            break\n",
        "        \n",
        "        elif user_input.lower() == \"/memory\":\n",
        "            clara_stats()\n",
        "            continue\n",
        "        \n",
        "        elif user_input.lower().startswith(\"/recall \"):\n",
        "            query = user_input[8:].strip()\n",
        "            clara_recall(query)\n",
        "            continue\n",
        "        \n",
        "        elif user_input.lower() == \"/entities\":\n",
        "            clara_entities()\n",
        "            continue\n",
        "        \n",
        "        elif user_input.lower() == \"/debug\":\n",
        "            clara_debug(not clara_memory.debug)\n",
        "            continue\n",
        "        \n",
        "        elif user_input.lower() == \"/save\":\n",
        "            clara_save()\n",
        "            continue\n",
        "        \n",
        "        elif user_input.lower() == \"/clear\":\n",
        "            clara_memory.memories = []\n",
        "            clara_memory.memory_bundle = np.zeros(clara_memory.dim, dtype=np.float32)\n",
        "            clara_memory.entity_index = {}\n",
        "            clara_memory.current_turn = 0\n",
        "            print(\"Memory cleared!\")\n",
        "            continue\n",
        "        \n",
        "        # Regular conversation\n",
        "        response = clara(user_input)\n",
        "        print(f\"\\nü§ñ Clara: {response}\")\n",
        "        \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nSession interrupted.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Save Memory & Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE MEMORY & CLEANUP\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SAVING & CLEANUP\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Final stats\n",
        "clara_stats()\n",
        "clara_entities()\n",
        "\n",
        "# Save memory\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Saving memory...\")\n",
        "clara_save()\n",
        "\n",
        "# Verify save\n",
        "if os.path.exists(MEMORY_FILE):\n",
        "    size = os.path.getsize(MEMORY_FILE) / 1024\n",
        "    print(f\"\\n‚úÖ Memory saved successfully!\")\n",
        "    print(f\"   File: {MEMORY_FILE}\")\n",
        "    print(f\"   Size: {size:.1f} KB\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Failed to save memory\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SESSION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nNext time you run this notebook, Clara will remember!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
