{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLARA + HDC MEMORY INTEGRATION\n",
        "\n",
        "This notebook integrates Hyperdimensional Computing (HDC) memory into Clara's dual-brain architecture.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Trained models in Google Drive (`/Lily/models/`)\n",
        "- `clara-knowledge` (Phi-3 merged)\n",
        "- `mistral_warmth`, `mistral_playful`, `mistral_encouragement` adapters\n",
        "\n",
        "**What this adds:**\n",
        "- HDC Memory System (~200KB footprint)\n",
        "- Memory-augmented routing\n",
        "- Interaction history storage\n",
        "- Personality vectors\n",
        "- Memory persistence (save/load)\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "User Query ‚Üí HDC Memory Context ‚Üí Semantic Router ‚Üí Brain Selection ‚Üí Response\n",
        "                    ‚Üë                                                    ‚îÇ\n",
        "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Store Interaction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# SETUP\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q peft sentence-transformers\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SETUP COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Check Existing Models (Safety Check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# SAFETY CHECK - Verify existing models without overwriting\n",
        "# ============================================================\n",
        "\n",
        "MODELS_DIR = \"/content/drive/MyDrive/Lily/models\"\n",
        "MEMORY_DIR = \"/content/drive/MyDrive/Lily/memory\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL VERIFICATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Required models\n",
        "required_models = {\n",
        "    \"clara-knowledge\": \"Phi-3 merged knowledge brain\",\n",
        "    \"mistral_warmth\": \"Personality adapter (warmth)\",\n",
        "    \"mistral_playful\": \"Personality adapter (playful)\", \n",
        "    \"mistral_encouragement\": \"Personality adapter (encouragement)\",\n",
        "}\n",
        "\n",
        "all_present = True\n",
        "model_status = {}\n",
        "\n",
        "print(\"\\nüìÅ Checking models in:\", MODELS_DIR)\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for model_name, description in required_models.items():\n",
        "    path = os.path.join(MODELS_DIR, model_name)\n",
        "    \n",
        "    # Check for key files that indicate a valid model\n",
        "    if model_name == \"clara-knowledge\":\n",
        "        # Merged model should have config.json\n",
        "        check_file = os.path.join(path, \"config.json\")\n",
        "    else:\n",
        "        # LoRA adapters have adapter_config.json\n",
        "        check_file = os.path.join(path, \"adapter_config.json\")\n",
        "    \n",
        "    exists = os.path.exists(check_file)\n",
        "    model_status[model_name] = exists\n",
        "    \n",
        "    if exists:\n",
        "        # Get size\n",
        "        total_size = sum(\n",
        "            os.path.getsize(os.path.join(path, f))\n",
        "            for f in os.listdir(path)\n",
        "            if os.path.isfile(os.path.join(path, f))\n",
        "        ) / 1e9\n",
        "        print(f\"  ‚úÖ {model_name:<25} ({total_size:.2f} GB)\")\n",
        "        print(f\"      ‚îî‚îÄ {description}\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå {model_name:<25} MISSING!\")\n",
        "        print(f\"      ‚îî‚îÄ {description}\")\n",
        "        all_present = False\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if all_present:\n",
        "    print(\"\\n‚úÖ All required models found!\")\n",
        "    print(\"   Models will NOT be overwritten.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some models are missing!\")\n",
        "    print(\"   Please run your training notebook first.\")\n",
        "    print(\"   This notebook requires pre-trained models.\")\n",
        "\n",
        "# Check/create memory directory\n",
        "print(\"\\nüìÅ Memory directory:\", MEMORY_DIR)\n",
        "if not os.path.exists(MEMORY_DIR):\n",
        "    os.makedirs(MEMORY_DIR)\n",
        "    print(\"   Created new memory directory\")\n",
        "else:\n",
        "    # Check for existing memory files\n",
        "    memory_files = [f for f in os.listdir(MEMORY_DIR) if f.endswith('.json')]\n",
        "    if memory_files:\n",
        "        print(f\"   Found {len(memory_files)} existing memory file(s):\")\n",
        "        for mf in memory_files:\n",
        "            size = os.path.getsize(os.path.join(MEMORY_DIR, mf)) / 1024\n",
        "            print(f\"      ‚îî‚îÄ {mf} ({size:.1f} KB)\")\n",
        "    else:\n",
        "        print(\"   No existing memory files (fresh start)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Load Semantic Router (Embedder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# SEMANTIC ROUTER - Load embedder and domain descriptions\n",
        "# ============================================================\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING SEMANTIC ROUTER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1. Loading embedding model...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"   ‚úÖ all-MiniLM-L6-v2 loaded\")\n",
        "\n",
        "# Domain descriptions (refined from your notebook)\n",
        "DOMAIN_DESCRIPTIONS = {\n",
        "    \"medical\": \"\"\"\n",
        "        symptoms diagnosis treatment disease illness pain fever infection\n",
        "        headache nauseous dizzy blood pressure heart lungs brain body\n",
        "        doctor hospital medicine medication prescription surgery vaccine\n",
        "        virus bacteria immune system allergies chronic acute patient health\n",
        "        tired fatigue exhausted sleep insomnia rash swollen sore throat\n",
        "        cough breathing chest stomach ache injury wound bleeding\n",
        "    \"\"\",\n",
        "    \n",
        "    \"coding\": \"\"\"\n",
        "        programming code software python javascript java function method\n",
        "        variable array list dictionary tuple loop error exception bug debug\n",
        "        API database SQL server backend frontend algorithm data structure\n",
        "        class object inheritance compile runtime syntax IndexError TypeError\n",
        "        iterate parse return import library framework git repository\n",
        "        crash deploy package module script terminal command line\n",
        "        Flask Django React Node npm pip install developer\n",
        "    \"\"\",\n",
        "    \n",
        "    \"teaching\": \"\"\"\n",
        "        explain how does work basics fundamentals introduction tutorial\n",
        "        step by step concept theory lesson learn teach education student\n",
        "        example analogy walk through ELI5 for dummies guide overview\n",
        "        what is the difference between simple explanation textbook\n",
        "        homework assignment class course study\n",
        "    \"\"\",\n",
        "    \n",
        "    \"quantum\": \"\"\"\n",
        "        quantum physics qubit superposition entanglement wave function\n",
        "        particle measurement collapse observer Schrodinger Heisenberg\n",
        "        quantum computer quantum gate Hadamard CNOT quantum circuit\n",
        "        coherence decoherence probability amplitude interference\n",
        "        quantum mechanics quantum state Planck photon electron spin\n",
        "        two places at once two states uncertainty principle\n",
        "        parallel universes both alive and dead particle wave duality\n",
        "    \"\"\",\n",
        "    \n",
        "    \"personality\": \"\"\"\n",
        "        feeling emotion mood happy sad angry anxious worried stressed\n",
        "        excited nervous scared lonely depressed overwhelmed frustrated\n",
        "        relationship friend family love support talk vent chat\n",
        "        my day rough tough great amazing terrible celebrate\n",
        "        broke up promotion new job interview date\n",
        "        grateful appreciate thankful thanks thank you\n",
        "        hey hi hello how are you good morning good night\n",
        "        nobody understands me moving away miss you\n",
        "        sorry to hear congratulations best friend\n",
        "        take care see you later nice to meet you\n",
        "    \"\"\",\n",
        "}\n",
        "\n",
        "print(\"\\n2. Computing domain embeddings...\")\n",
        "domain_embeddings = {}\n",
        "for domain, description in DOMAIN_DESCRIPTIONS.items():\n",
        "    clean_desc = \" \".join(description.split())\n",
        "    domain_embeddings[domain] = embedder.encode(clean_desc)\n",
        "    print(f\"   ‚úÖ {domain}\")\n",
        "\n",
        "print(\"\\n‚úÖ Semantic router ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: HDC Memory System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# HDC MEMORY SYSTEM\n",
        "# ============================================================\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import time\n",
        "import json\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"HDC MEMORY SYSTEM\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "@dataclass\n",
        "class Memory:\n",
        "    \"\"\"Single memory unit\"\"\"\n",
        "    text: str\n",
        "    timestamp: float\n",
        "    memory_type: str  # 'interaction', 'preference', 'fact'\n",
        "    importance: float = 0.5\n",
        "    domain: str = \"general\"\n",
        "\n",
        "\n",
        "class ClaraHDCMemory:\n",
        "    \"\"\"\n",
        "    Hyperdimensional Computing Memory System for Clara\n",
        "    \n",
        "    Features:\n",
        "    - Reuses existing embedder (no additional model loading)\n",
        "    - 10,000-dimension bipolar hypervectors\n",
        "    - Structured binding for compositional memories\n",
        "    - Personality vectors influence routing\n",
        "    - ~200KB footprint (excluding embedder)\n",
        "    \n",
        "    HDC Operations:\n",
        "    - Bind (‚äó): Element-wise multiplication - creates associations\n",
        "    - Bundle (+): Element-wise addition + sign - superposition\n",
        "    - Similarity: Cosine similarity for retrieval\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embedder, dim: int = 10000, seed: int = 42):\n",
        "        self.dim = dim\n",
        "        self.embedder = embedder  # Reuse existing embedder\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        \n",
        "        # Random projection matrix: 384-dim ‚Üí 10K-dim\n",
        "        print(\"   Initializing projection matrix...\")\n",
        "        self.projection = self.rng.randn(384, dim).astype(np.float32)\n",
        "        self.projection /= np.linalg.norm(self.projection, axis=1, keepdims=True)\n",
        "        \n",
        "        # Memory stores\n",
        "        self.memories: List[Tuple[np.ndarray, Memory]] = []\n",
        "        self.memory_bundle = np.zeros(dim, dtype=np.float32)\n",
        "        \n",
        "        # Symbol library for structured binding\n",
        "        print(\"   Building symbol library...\")\n",
        "        self.symbols: Dict[str, np.ndarray] = {}\n",
        "        self._init_symbols()\n",
        "        \n",
        "        # Personality vectors\n",
        "        print(\"   Encoding personality vectors...\")\n",
        "        self.personality = self._init_personality()\n",
        "        \n",
        "        print(f\"   ‚úÖ HDC Memory initialized (dim={dim})\")\n",
        "    \n",
        "    def _init_symbols(self):\n",
        "        \"\"\"Create base symbol vocabulary\"\"\"\n",
        "        base_symbols = [\n",
        "            # Roles\n",
        "            \"ROLE_USER\", \"ROLE_CLARA\", \"ROLE_TOPIC\", \"ROLE_OUTCOME\",\n",
        "            # Actions  \n",
        "            \"ASKED\", \"ANSWERED\", \"STRUGGLED\", \"SUCCEEDED\", \"PREFERRED\",\n",
        "            # Domains (match router)\n",
        "            \"MEDICAL\", \"CODING\", \"TEACHING\", \"QUANTUM\", \"PERSONALITY\",\n",
        "            # Outcomes\n",
        "            \"HELPFUL\", \"CONFUSED\", \"SATISFIED\", \"FRUSTRATED\",\n",
        "            # Time markers\n",
        "            \"RECENT\", \"TODAY\", \"THIS_SESSION\"\n",
        "        ]\n",
        "        for s in base_symbols:\n",
        "            self.symbols[s] = self._random_hv()\n",
        "    \n",
        "    def _init_personality(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Encode Clara's personality traits as hypervectors\"\"\"\n",
        "        traits = {\n",
        "            'warmth': 0.85,\n",
        "            'curiosity': 0.75,\n",
        "            'patience': 0.90,\n",
        "            'encouragement': 0.80,\n",
        "        }\n",
        "        personality = {}\n",
        "        for trait, strength in traits.items():\n",
        "            base_hv = self._random_hv()\n",
        "            personality[trait] = (base_hv * strength).astype(np.float32)\n",
        "        \n",
        "        # Composite personality vector\n",
        "        all_traits = list(personality.values())\n",
        "        personality['composite'] = np.sign(\n",
        "            np.sum(all_traits, axis=0)\n",
        "        ).astype(np.float32)\n",
        "        \n",
        "        return personality\n",
        "    \n",
        "    def _random_hv(self) -> np.ndarray:\n",
        "        \"\"\"Generate random bipolar hypervector {-1, +1}\"\"\"\n",
        "        return self.rng.choice([-1, 1], size=self.dim).astype(np.float32)\n",
        "    \n",
        "    def _text_to_hv(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Convert text to hypervector using existing embedder\"\"\"\n",
        "        embedding = self.embedder.encode(text)\n",
        "        hv = embedding @ self.projection\n",
        "        return np.sign(hv).astype(np.float32)\n",
        "    \n",
        "    def _get_symbol(self, name: str) -> np.ndarray:\n",
        "        \"\"\"Get or create symbol\"\"\"\n",
        "        name_upper = name.upper()\n",
        "        if name_upper not in self.symbols:\n",
        "            self.symbols[name_upper] = self._random_hv()\n",
        "        return self.symbols[name_upper]\n",
        "    \n",
        "    # === HDC Operations ===\n",
        "    \n",
        "    def bind(self, hv1: np.ndarray, hv2: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Bind two hypervectors (‚äó) - creates association\"\"\"\n",
        "        return hv1 * hv2\n",
        "    \n",
        "    def bundle(self, hvs: List[np.ndarray]) -> np.ndarray:\n",
        "        \"\"\"Bundle hypervectors (+) - superposition\"\"\"\n",
        "        if not hvs:\n",
        "            return np.zeros(self.dim, dtype=np.float32)\n",
        "        return np.sign(np.sum(hvs, axis=0)).astype(np.float32)\n",
        "    \n",
        "    def similarity(self, hv1: np.ndarray, hv2: np.ndarray) -> float:\n",
        "        \"\"\"Cosine similarity between hypervectors\"\"\"\n",
        "        n1, n2 = np.linalg.norm(hv1), np.linalg.norm(hv2)\n",
        "        if n1 == 0 or n2 == 0:\n",
        "            return 0.0\n",
        "        return float(np.dot(hv1, hv2) / (n1 * n2))\n",
        "    \n",
        "    # === Memory Operations ===\n",
        "    \n",
        "    def store(self, text: str, memory_type: str = \"interaction\",\n",
        "              importance: float = 0.5, domain: str = \"general\",\n",
        "              **bindings) -> None:\n",
        "        \"\"\"\n",
        "        Store a memory with optional structured bindings\n",
        "        \n",
        "        Args:\n",
        "            text: The memory content\n",
        "            memory_type: 'interaction', 'preference', 'fact'\n",
        "            importance: 0.0-1.0 importance score\n",
        "            domain: 'medical', 'coding', 'teaching', 'quantum', 'personality'\n",
        "            **bindings: Structured role-filler pairs\n",
        "            \n",
        "        Example:\n",
        "            memory.store(\"User asked about async Python\",\n",
        "                        memory_type=\"interaction\",\n",
        "                        domain=\"coding\",\n",
        "                        topic=\"ASYNC_PYTHON\", outcome=\"ANSWERED\")\n",
        "        \"\"\"\n",
        "        # Create semantic hypervector\n",
        "        text_hv = self._text_to_hv(text)\n",
        "        \n",
        "        # Add domain binding\n",
        "        domain_hv = self._get_symbol(domain.upper())\n",
        "        text_hv = self.bind(text_hv, domain_hv)\n",
        "        \n",
        "        # Add structural bindings if provided\n",
        "        if bindings:\n",
        "            bound_parts = []\n",
        "            for role, filler in bindings.items():\n",
        "                role_hv = self._get_symbol(f\"ROLE_{role.upper()}\")\n",
        "                filler_hv = self._get_symbol(str(filler).upper())\n",
        "                bound_parts.append(self.bind(role_hv, filler_hv))\n",
        "            if bound_parts:\n",
        "                structure_hv = self.bundle(bound_parts)\n",
        "                text_hv = self.bundle([text_hv, structure_hv])\n",
        "        \n",
        "        memory = Memory(\n",
        "            text=text,\n",
        "            timestamp=time.time(),\n",
        "            memory_type=memory_type,\n",
        "            importance=importance,\n",
        "            domain=domain\n",
        "        )\n",
        "        \n",
        "        self.memories.append((text_hv, memory))\n",
        "        \n",
        "        # Update bundled representation\n",
        "        self.memory_bundle = self.bundle([self.memory_bundle, text_hv])\n",
        "    \n",
        "    def recall(self, query: str, top_k: int = 3,\n",
        "               domain_filter: Optional[str] = None) -> List[Tuple[Memory, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve memories similar to query\n",
        "        \n",
        "        Args:\n",
        "            query: Search query\n",
        "            top_k: Number of results\n",
        "            domain_filter: Optional domain to filter by\n",
        "            \n",
        "        Returns:\n",
        "            List of (Memory, similarity_score) tuples\n",
        "        \"\"\"\n",
        "        if not self.memories:\n",
        "            return []\n",
        "        \n",
        "        query_hv = self._text_to_hv(query)\n",
        "        \n",
        "        results = []\n",
        "        for hv, memory in self.memories:\n",
        "            # Apply domain filter if specified\n",
        "            if domain_filter and memory.domain != domain_filter:\n",
        "                continue\n",
        "            \n",
        "            sim = self.similarity(query_hv, hv)\n",
        "            \n",
        "            # Weight by importance and recency\n",
        "            age_hours = (time.time() - memory.timestamp) / 3600\n",
        "            recency = 1.0 / (1.0 + age_hours / 24)\n",
        "            weighted = sim * (0.7 + 0.3 * memory.importance) * (0.8 + 0.2 * recency)\n",
        "            \n",
        "            results.append((memory, weighted))\n",
        "        \n",
        "        results.sort(key=lambda x: x[1], reverse=True)\n",
        "        return results[:top_k]\n",
        "    \n",
        "    def get_context_for_routing(self, query: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get memory-augmented context for routing decisions\n",
        "        \n",
        "        Combines:\n",
        "        - Query semantics\n",
        "        - Relevant memory context\n",
        "        - Personality alignment\n",
        "        \"\"\"\n",
        "        query_hv = self._text_to_hv(query)\n",
        "        \n",
        "        # Get relevant memory context\n",
        "        relevant = self.recall(query, top_k=3)\n",
        "        if relevant:\n",
        "            memory_hvs = [self._text_to_hv(m.text) for m, _ in relevant]\n",
        "            memory_context = self.bundle(memory_hvs)\n",
        "        else:\n",
        "            memory_context = np.zeros(self.dim, dtype=np.float32)\n",
        "        \n",
        "        # Combine: query + memory + personality\n",
        "        routing_hv = self.bundle([\n",
        "            query_hv,\n",
        "            memory_context * 0.3,\n",
        "            self.personality['composite'] * 0.2\n",
        "        ])\n",
        "        \n",
        "        return routing_hv\n",
        "    \n",
        "    def get_context_string(self, query: str, max_memories: int = 2) -> str:\n",
        "        \"\"\"\n",
        "        Get memory context as string for prompt injection\n",
        "        \n",
        "        Returns formatted string to include in LLM prompt\n",
        "        \"\"\"\n",
        "        relevant = self.recall(query, top_k=max_memories)\n",
        "        if not relevant:\n",
        "            return \"\"\n",
        "        \n",
        "        # Only include memories above threshold\n",
        "        good_memories = [(m, s) for m, s in relevant if s > 0.25]\n",
        "        if not good_memories:\n",
        "            return \"\"\n",
        "        \n",
        "        context_parts = [\"[Previous context:\"]\n",
        "        for mem, score in good_memories:\n",
        "            context_parts.append(f\"- {mem.text[:80]}\")\n",
        "        context_parts.append(\"]\")\n",
        "        \n",
        "        return \"\\n\".join(context_parts)\n",
        "    \n",
        "    def get_domain_history(self, domain: str) -> List[Memory]:\n",
        "        \"\"\"Get all memories for a specific domain\"\"\"\n",
        "        return [m for _, m in self.memories if m.domain == domain]\n",
        "    \n",
        "    def size_bytes(self) -> int:\n",
        "        \"\"\"Memory footprint (excluding embedder)\"\"\"\n",
        "        bundle_size = self.memory_bundle.nbytes\n",
        "        memories_size = sum(hv.nbytes for hv, _ in self.memories)\n",
        "        symbols_size = sum(hv.nbytes for hv in self.symbols.values())\n",
        "        personality_size = sum(hv.nbytes for hv in self.personality.values())\n",
        "        projection_size = self.projection.nbytes\n",
        "        return bundle_size + memories_size + symbols_size + personality_size + projection_size\n",
        "    \n",
        "    def stats(self) -> Dict:\n",
        "        \"\"\"Get memory statistics\"\"\"\n",
        "        domain_counts = {}\n",
        "        for _, m in self.memories:\n",
        "            domain_counts[m.domain] = domain_counts.get(m.domain, 0) + 1\n",
        "        \n",
        "        return {\n",
        "            \"total_memories\": len(self.memories),\n",
        "            \"by_domain\": domain_counts,\n",
        "            \"symbols\": len(self.symbols),\n",
        "            \"size_kb\": self.size_bytes() / 1024\n",
        "        }\n",
        "    \n",
        "    # === Persistence ===\n",
        "    \n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save memory state to file\"\"\"\n",
        "        data = {\n",
        "            'version': '1.0',\n",
        "            'dim': self.dim,\n",
        "            'memories': [\n",
        "                {\n",
        "                    'hv': hv.tolist(),\n",
        "                    'text': m.text,\n",
        "                    'timestamp': m.timestamp,\n",
        "                    'memory_type': m.memory_type,\n",
        "                    'importance': m.importance,\n",
        "                    'domain': m.domain\n",
        "                }\n",
        "                for hv, m in self.memories\n",
        "            ],\n",
        "            'symbols': {k: v.tolist() for k, v in self.symbols.items()},\n",
        "            'memory_bundle': self.memory_bundle.tolist(),\n",
        "            'projection': self.projection.tolist()\n",
        "        }\n",
        "        \n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(data, f)\n",
        "        \n",
        "        print(f\"‚úÖ Saved {len(self.memories)} memories to {path}\")\n",
        "        print(f\"   Size: {os.path.getsize(path) / 1024:.1f} KB\")\n",
        "    \n",
        "    def load(self, path: str):\n",
        "        \"\"\"Load memory state from file\"\"\"\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"‚ö†Ô∏è No memory file found at {path}\")\n",
        "            return False\n",
        "        \n",
        "        with open(path) as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # Validate version and dimension\n",
        "        if data.get('dim') != self.dim:\n",
        "            print(f\"‚ö†Ô∏è Dimension mismatch: file has {data.get('dim')}, expected {self.dim}\")\n",
        "            return False\n",
        "        \n",
        "        # Load memories\n",
        "        self.memories = [\n",
        "            (\n",
        "                np.array(m['hv'], dtype=np.float32),\n",
        "                Memory(\n",
        "                    text=m['text'],\n",
        "                    timestamp=m['timestamp'],\n",
        "                    memory_type=m['memory_type'],\n",
        "                    importance=m['importance'],\n",
        "                    domain=m.get('domain', 'general')\n",
        "                )\n",
        "            )\n",
        "            for m in data['memories']\n",
        "        ]\n",
        "        \n",
        "        # Load symbols\n",
        "        self.symbols = {\n",
        "            k: np.array(v, dtype=np.float32)\n",
        "            for k, v in data['symbols'].items()\n",
        "        }\n",
        "        \n",
        "        # Load bundle\n",
        "        self.memory_bundle = np.array(data['memory_bundle'], dtype=np.float32)\n",
        "        \n",
        "        # Load projection if present (for consistency)\n",
        "        if 'projection' in data:\n",
        "            self.projection = np.array(data['projection'], dtype=np.float32)\n",
        "        \n",
        "        print(f\"‚úÖ Loaded {len(self.memories)} memories from {path}\")\n",
        "        return True\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Initialize HDC Memory\n",
        "# ============================================================\n",
        "print(\"\\nInitializing Clara's HDC Memory...\")\n",
        "clara_memory = ClaraHDCMemory(embedder=embedder, dim=10000)\n",
        "\n",
        "# Try to load existing memory\n",
        "MEMORY_FILE = os.path.join(MEMORY_DIR, \"clara_memory.json\")\n",
        "if os.path.exists(MEMORY_FILE):\n",
        "    print(f\"\\nFound existing memory file, loading...\")\n",
        "    clara_memory.load(MEMORY_FILE)\n",
        "else:\n",
        "    print(f\"\\nNo existing memory file - starting fresh\")\n",
        "\n",
        "print(f\"\\nüìä Memory Stats:\")\n",
        "stats = clara_memory.stats()\n",
        "print(f\"   Total memories: {stats['total_memories']}\")\n",
        "print(f\"   Symbols: {stats['symbols']}\")\n",
        "print(f\"   Size: {stats['size_kb']:.1f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Smart Router with Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# SMART ROUTER (with memory integration)\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SMART ROUTER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def smart_route(query: str, use_memory: bool = True, \n",
        "                threshold: float = 0.20) -> tuple:\n",
        "    \"\"\"\n",
        "    Route query using semantic similarity + memory context\n",
        "    \n",
        "    Args:\n",
        "        query: User input\n",
        "        use_memory: Whether to incorporate memory context\n",
        "        threshold: Minimum confidence threshold\n",
        "        \n",
        "    Returns:\n",
        "        (brain_type, domain, confidence, memory_context_used)\n",
        "    \"\"\"\n",
        "    # Get query embedding\n",
        "    query_embedding = embedder.encode(query)\n",
        "    \n",
        "    # Calculate similarities to each domain\n",
        "    similarities = {}\n",
        "    for domain, domain_emb in domain_embeddings.items():\n",
        "        similarity = np.dot(query_embedding, domain_emb) / (\n",
        "            np.linalg.norm(query_embedding) * np.linalg.norm(domain_emb)\n",
        "        )\n",
        "        similarities[domain] = similarity\n",
        "    \n",
        "    # Memory-based boosting\n",
        "    memory_context_used = False\n",
        "    if use_memory and len(clara_memory.memories) > 0:\n",
        "        relevant = clara_memory.recall(query, top_k=2)\n",
        "        if relevant:\n",
        "            for mem, score in relevant:\n",
        "                if score > 0.3 and mem.domain in similarities:\n",
        "                    # Boost the domain from memory\n",
        "                    similarities[mem.domain] += 0.05\n",
        "                    memory_context_used = True\n",
        "    \n",
        "    # Sort by similarity\n",
        "    sorted_domains = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "    best_domain, best_conf = sorted_domains[0]\n",
        "    second_domain, second_conf = sorted_domains[1]\n",
        "    \n",
        "    # If top two are very close and one is personality, prefer personality\n",
        "    # (Clara should be warm/supportive when uncertain)\n",
        "    if best_conf - second_conf < 0.05:\n",
        "        if second_domain == \"personality\":\n",
        "            best_domain = \"personality\"\n",
        "            best_conf = second_conf\n",
        "    \n",
        "    # Low confidence fallback\n",
        "    if best_conf < threshold and best_domain != \"personality\":\n",
        "        if similarities[\"personality\"] > 0.15:\n",
        "            best_domain = \"personality\"\n",
        "            best_conf = similarities[\"personality\"]\n",
        "    \n",
        "    # Determine brain type\n",
        "    if best_domain == \"personality\":\n",
        "        brain = \"personality\"\n",
        "        domain = \"warmth\"  # Default adapter\n",
        "    else:\n",
        "        brain = \"knowledge\"\n",
        "        domain = best_domain\n",
        "    \n",
        "    return brain, domain, best_conf, memory_context_used\n",
        "\n",
        "\n",
        "def clean_response(response: str) -> str:\n",
        "    \"\"\"Clean up response artifacts\"\"\"\n",
        "    stop_markers = [\"### Instruction:\", \"Instruction:\", \"\\n\\n\\n\", \"User:\"]\n",
        "    for marker in stop_markers:\n",
        "        if marker in response:\n",
        "            response = response.split(marker)[0].strip()\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "print(\"‚úÖ Router functions defined\")\n",
        "\n",
        "# Quick test\n",
        "print(\"\\nüìã Router test:\")\n",
        "test_queries = [\n",
        "    \"How do I read a CSV in Python?\",\n",
        "    \"I'm feeling stressed about work\",\n",
        "    \"What is quantum entanglement?\"\n",
        "]\n",
        "for q in test_queries:\n",
        "    brain, domain, conf, mem_used = smart_route(q)\n",
        "    mem_icon = \"üß†\" if mem_used else \"  \"\n",
        "    print(f\"   {mem_icon} {q[:40]:40} ‚Üí {brain}/{domain} ({conf:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Load Clara's Brains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# LOAD CLARA'S DUAL-BRAIN SYSTEM\n",
        "# ============================================================\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING CLARA'S BRAINS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# KNOWLEDGE BRAIN (Phi-3 merged)\n",
        "# ============================================================\n",
        "print(\"\\n1. Loading Knowledge Brain (Phi-3)...\")\n",
        "\n",
        "knowledge_path = os.path.join(MODELS_DIR, \"clara-knowledge\")\n",
        "\n",
        "if not os.path.exists(knowledge_path):\n",
        "    raise FileNotFoundError(f\"Knowledge model not found: {knowledge_path}\")\n",
        "\n",
        "knowledge_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    knowledge_path,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "knowledge_model = AutoModelForCausalLM.from_pretrained(\n",
        "    knowledge_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "knowledge_model.eval()\n",
        "print(\"   ‚úÖ Knowledge brain loaded\")\n",
        "\n",
        "# ============================================================\n",
        "# PERSONALITY BRAIN (Mistral + LoRA adapters)\n",
        "# ============================================================\n",
        "print(\"\\n2. Loading Personality Brain (Mistral + adapters)...\")\n",
        "\n",
        "personality_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        ")\n",
        "\n",
        "personality_base = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load adapters\n",
        "adapters_to_load = [\n",
        "    (\"warmth\", \"mistral_warmth\"),\n",
        "    (\"playful\", \"mistral_playful\"),\n",
        "    (\"encouragement\", \"mistral_encouragement\"),\n",
        "]\n",
        "\n",
        "# Load first adapter\n",
        "first_name, first_path = adapters_to_load[0]\n",
        "personality_model = PeftModel.from_pretrained(\n",
        "    personality_base,\n",
        "    os.path.join(MODELS_DIR, first_path),\n",
        "    adapter_name=first_name\n",
        ")\n",
        "\n",
        "# Load remaining adapters\n",
        "for adapter_name, adapter_path in adapters_to_load[1:]:\n",
        "    full_path = os.path.join(MODELS_DIR, adapter_path)\n",
        "    if os.path.exists(full_path):\n",
        "        personality_model.load_adapter(full_path, adapter_name=adapter_name)\n",
        "        print(f\"   ‚úÖ Loaded adapter: {adapter_name}\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Adapter not found: {adapter_path}\")\n",
        "\n",
        "personality_model.set_adapter(\"warmth\")  # Default\n",
        "personality_model.eval()\n",
        "print(\"   ‚úÖ Personality brain loaded\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üß† CLARA'S BRAINS ARE READY!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Clara Main Interface (with Memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# CLARA - Main Interface with HDC Memory\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CLARA - MAIN INTERFACE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def clara(query: str, verbose: bool = True, \n",
        "          use_memory: bool = True,\n",
        "          store_interaction: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Clara's main interface - now with memory!\n",
        "    \n",
        "    Args:\n",
        "        query: User input\n",
        "        verbose: Print routing info\n",
        "        use_memory: Use memory for context\n",
        "        store_interaction: Store this interaction in memory\n",
        "        \n",
        "    Returns:\n",
        "        Clara's response\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Route the query\n",
        "    brain, domain, conf, mem_used = smart_route(query, use_memory=use_memory)\n",
        "    \n",
        "    if verbose:\n",
        "        mem_icon = \"üß†\" if mem_used else \"  \"\n",
        "        print(f\"   {mem_icon} Routing: {brain}/{domain} (conf: {conf:.2f})\")\n",
        "    \n",
        "    # 2. Get memory context for prompt\n",
        "    memory_context = \"\"\n",
        "    if use_memory:\n",
        "        memory_context = clara_memory.get_context_string(query)\n",
        "        if memory_context and verbose:\n",
        "            print(f\"   üìö Memory context included\")\n",
        "    \n",
        "    # 3. Generate response\n",
        "    if brain == \"knowledge\":\n",
        "        # Knowledge brain (Phi-3)\n",
        "        if memory_context:\n",
        "            prompt = f\"### Instruction:\\n{memory_context}\\n\\nUser question: {query}\\n\\n### Response:\\n\"\n",
        "        else:\n",
        "            prompt = f\"### Instruction:\\n{query}\\n\\n### Response:\\n\"\n",
        "        \n",
        "        inputs = knowledge_tokenizer(prompt, return_tensors=\"pt\").to(knowledge_model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = knowledge_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=250,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=knowledge_tokenizer.eos_token_id,\n",
        "                use_cache=False  # Phi-3 compatibility\n",
        "            )\n",
        "        \n",
        "        response = knowledge_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if \"### Response:\" in response:\n",
        "            response = response.split(\"### Response:\")[-1].strip()\n",
        "    \n",
        "    else:\n",
        "        # Personality brain (Mistral + adapter)\n",
        "        personality_model.set_adapter(domain)\n",
        "        \n",
        "        if memory_context:\n",
        "            prompt = f\"### Instruction:\\n{memory_context}\\n\\nUser message: {query}\\n\\n### Response:\\n\"\n",
        "        else:\n",
        "            prompt = f\"### Instruction:\\n{query}\\n\\n### Response:\\n\"\n",
        "        \n",
        "        inputs = personality_tokenizer(prompt, return_tensors=\"pt\").to(personality_model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = personality_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=150,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=personality_tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        response = personality_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if \"### Response:\" in response:\n",
        "            response = response.split(\"### Response:\")[-1].strip()\n",
        "    \n",
        "    # Clean response\n",
        "    response = clean_response(response)\n",
        "    \n",
        "    # 4. Store interaction in memory\n",
        "    if store_interaction:\n",
        "        # Create summary of interaction\n",
        "        interaction_summary = f\"User asked about {domain}: {query[:50]}...\"\n",
        "        \n",
        "        clara_memory.store(\n",
        "            text=interaction_summary,\n",
        "            memory_type=\"interaction\",\n",
        "            importance=0.4 + (conf * 0.4),  # Higher confidence = more important\n",
        "            domain=domain if brain == \"knowledge\" else \"personality\",\n",
        "            topic=domain.upper(),\n",
        "            outcome=\"ANSWERED\"\n",
        "        )\n",
        "    \n",
        "    return response\n",
        "\n",
        "\n",
        "def clara_remember(text: str, importance: float = 0.7, \n",
        "                   domain: str = \"general\") -> None:\n",
        "    \"\"\"\n",
        "    Explicitly store something in Clara's memory\n",
        "    \n",
        "    Example:\n",
        "        clara_remember(\"User prefers visual explanations\", \n",
        "                      importance=0.9, domain=\"teaching\")\n",
        "    \"\"\"\n",
        "    clara_memory.store(\n",
        "        text=text,\n",
        "        memory_type=\"preference\",\n",
        "        importance=importance,\n",
        "        domain=domain\n",
        "    )\n",
        "    print(f\"‚úÖ Stored: {text[:50]}...\")\n",
        "\n",
        "\n",
        "def clara_recall(query: str, top_k: int = 5) -> None:\n",
        "    \"\"\"\n",
        "    Query Clara's memory directly\n",
        "    \"\"\"\n",
        "    results = clara_memory.recall(query, top_k=top_k)\n",
        "    \n",
        "    print(f\"\\nüîç Memory search: '{query}'\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if not results:\n",
        "        print(\"   No memories found\")\n",
        "        return\n",
        "    \n",
        "    for mem, score in results:\n",
        "        print(f\"   [{score:.2f}] [{mem.domain:10}] {mem.text[:50]}...\")\n",
        "\n",
        "\n",
        "def clara_stats() -> None:\n",
        "    \"\"\"Show Clara's memory statistics\"\"\"\n",
        "    stats = clara_memory.stats()\n",
        "    \n",
        "    print(\"\\nüìä Clara's Memory Stats\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"   Total memories: {stats['total_memories']}\")\n",
        "    print(f\"   Symbols: {stats['symbols']}\")\n",
        "    print(f\"   Size: {stats['size_kb']:.1f} KB\")\n",
        "    \n",
        "    if stats['by_domain']:\n",
        "        print(\"\\n   By domain:\")\n",
        "        for domain, count in sorted(stats['by_domain'].items()):\n",
        "            print(f\"      {domain}: {count}\")\n",
        "\n",
        "\n",
        "def clara_save() -> None:\n",
        "    \"\"\"Save Clara's memory to disk\"\"\"\n",
        "    clara_memory.save(MEMORY_FILE)\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ Clara interface ready!\")\n",
        "print(\"\\nAvailable functions:\")\n",
        "print(\"   clara(query)           - Talk to Clara\")\n",
        "print(\"   clara_remember(text)   - Store something in memory\")\n",
        "print(\"   clara_recall(query)    - Search Clara's memory\")\n",
        "print(\"   clara_stats()          - Show memory statistics\")\n",
        "print(\"   clara_save()           - Save memory to disk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Test Clara with Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# TEST CLARA WITH MEMORY\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TESTING CLARA WITH HDC MEMORY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Show initial memory state\n",
        "clara_stats()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Test 1: Knowledge query (coding)\")\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nüë§ User: How do I read a CSV file in Python?\")\n",
        "response = clara(\"How do I read a CSV file in Python?\")\n",
        "print(f\"\\nü§ñ Clara: {response[:300]}...\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Test 2: Follow-up query (should use memory)\")\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nüë§ User: What about writing to a CSV?\")\n",
        "response = clara(\"What about writing to a CSV?\")\n",
        "print(f\"\\nü§ñ Clara: {response[:300]}...\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Test 3: Personality query\")\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nüë§ User: I'm feeling stressed about my project deadline\")\n",
        "response = clara(\"I'm feeling stressed about my project deadline\")\n",
        "print(f\"\\nü§ñ Clara: {response[:300]}...\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Test 4: Check memory\")\n",
        "print(\"-\" * 60)\n",
        "clara_recall(\"Python CSV\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Interactive Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# INTERACTIVE SESSION\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INTERACTIVE CLARA SESSION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nType your message to Clara.\")\n",
        "print(\"Special commands:\")\n",
        "print(\"   /memory     - Show memory stats\")\n",
        "print(\"   /recall X   - Search memory for X\")\n",
        "print(\"   /save       - Save memory to disk\")\n",
        "print(\"   /quit       - End session\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"\\nüë§ You: \").strip()\n",
        "        \n",
        "        if not user_input:\n",
        "            continue\n",
        "        \n",
        "        # Handle special commands\n",
        "        if user_input.lower() == \"/quit\":\n",
        "            print(\"\\nEnding session. Don't forget to save memory!\")\n",
        "            break\n",
        "        \n",
        "        elif user_input.lower() == \"/memory\":\n",
        "            clara_stats()\n",
        "            continue\n",
        "        \n",
        "        elif user_input.lower().startswith(\"/recall \"):\n",
        "            query = user_input[8:].strip()\n",
        "            clara_recall(query)\n",
        "            continue\n",
        "        \n",
        "        elif user_input.lower() == \"/save\":\n",
        "            clara_save()\n",
        "            continue\n",
        "        \n",
        "        # Regular conversation\n",
        "        response = clara(user_input)\n",
        "        print(f\"\\nü§ñ Clara: {response}\")\n",
        "        \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nSession interrupted.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Save Memory & Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# SAVE MEMORY & CLEANUP\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SAVING & CLEANUP\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Final stats\n",
        "clara_stats()\n",
        "\n",
        "# Save memory\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Saving memory...\")\n",
        "clara_save()\n",
        "\n",
        "# Verify save\n",
        "if os.path.exists(MEMORY_FILE):\n",
        "    size = os.path.getsize(MEMORY_FILE) / 1024\n",
        "    print(f\"\\n‚úÖ Memory saved successfully!\")\n",
        "    print(f\"   File: {MEMORY_FILE}\")\n",
        "    print(f\"   Size: {size:.1f} KB\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Failed to save memory\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SESSION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nNext time you run this notebook, Clara will remember!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Optional: HDC Router Experiment\n",
        "\n",
        "The cells below implement a pure HDC-based router as an alternative to the semantic router. \n",
        "This is experimental - use for comparison/research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# EXPERIMENTAL: Pure HDC Router\n",
        "# ============================================================\n",
        "\n",
        "class HDCRouter:\n",
        "    \"\"\"\n",
        "    Pure HDC-based routing (alternative to semantic router)\n",
        "    \n",
        "    Pros:\n",
        "    - Fully interpretable (can explain via symbol overlap)\n",
        "    - Compositional queries\n",
        "    - Personal context integrated\n",
        "    \n",
        "    Cons:\n",
        "    - Requires tuning symbol vocabulary\n",
        "    - May need more data to match semantic accuracy\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, memory: ClaraHDCMemory):\n",
        "        self.memory = memory\n",
        "        \n",
        "        # Expert signatures as hypervectors\n",
        "        print(\"Building expert signatures...\")\n",
        "        self.experts = {\n",
        "            \"medical\": self._create_expert_signature([\n",
        "                \"symptoms\", \"disease\", \"treatment\", \"health\", \n",
        "                \"doctor\", \"medicine\", \"pain\", \"diagnosis\"\n",
        "            ]),\n",
        "            \"coding\": self._create_expert_signature([\n",
        "                \"code\", \"programming\", \"python\", \"function\",\n",
        "                \"debug\", \"error\", \"software\", \"algorithm\"\n",
        "            ]),\n",
        "            \"teaching\": self._create_expert_signature([\n",
        "                \"explain\", \"learn\", \"understand\", \"concept\",\n",
        "                \"tutorial\", \"beginner\", \"example\", \"how\"\n",
        "            ]),\n",
        "            \"quantum\": self._create_expert_signature([\n",
        "                \"quantum\", \"qubit\", \"superposition\", \"entanglement\",\n",
        "                \"particle\", \"physics\", \"wave\", \"measurement\"\n",
        "            ]),\n",
        "            \"personality\": self._create_expert_signature([\n",
        "                \"feeling\", \"emotion\", \"stressed\", \"happy\",\n",
        "                \"sad\", \"support\", \"friend\", \"worried\"\n",
        "            ]),\n",
        "        }\n",
        "        print(\"‚úÖ HDC Router initialized\")\n",
        "    \n",
        "    def _create_expert_signature(self, keywords: list) -> np.ndarray:\n",
        "        \"\"\"Create expert signature by bundling keyword symbols\"\"\"\n",
        "        hvs = [self.memory._get_symbol(kw.upper()) for kw in keywords]\n",
        "        return self.memory.bundle(hvs)\n",
        "    \n",
        "    def route(self, query: str, use_memory: bool = True) -> tuple:\n",
        "        \"\"\"\n",
        "        Route using HDC similarity + memory context\n",
        "        \n",
        "        Returns: (brain, domain, confidence, explanation)\n",
        "        \"\"\"\n",
        "        # Encode query\n",
        "        query_hv = self.memory._text_to_hv(query)\n",
        "        \n",
        "        # Add personal context from memory\n",
        "        if use_memory and len(self.memory.memories) > 0:\n",
        "            memory_context = self.memory.get_context_for_routing(query)\n",
        "            routing_hv = self.memory.bundle([query_hv, memory_context * 0.3])\n",
        "        else:\n",
        "            routing_hv = query_hv\n",
        "        \n",
        "        # Compare to expert signatures\n",
        "        scores = {}\n",
        "        for expert, sig_hv in self.experts.items():\n",
        "            scores[expert] = self.memory.similarity(routing_hv, sig_hv)\n",
        "        \n",
        "        # Find best match\n",
        "        best = max(scores, key=scores.get)\n",
        "        brain = \"personality\" if best == \"personality\" else \"knowledge\"\n",
        "        domain = \"warmth\" if best == \"personality\" else best\n",
        "        \n",
        "        # Explanation (interpretability!)\n",
        "        explanation = {\n",
        "            \"all_scores\": scores,\n",
        "            \"memory_used\": use_memory and len(self.memory.memories) > 0\n",
        "        }\n",
        "        \n",
        "        return brain, domain, scores[best], explanation\n",
        "\n",
        "\n",
        "# Initialize HDC Router\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HDC ROUTER (Experimental)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "hdc_router = HDCRouter(clara_memory)\n",
        "\n",
        "# Compare routing\n",
        "print(\"\\nüìã Comparing routers:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Query':<35} {'Semantic':<15} {'HDC':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "test_queries = [\n",
        "    \"How do I read a CSV in Python?\",\n",
        "    \"I'm feeling anxious about tomorrow\",\n",
        "    \"What is quantum entanglement?\",\n",
        "    \"Explain how neural networks learn\",\n",
        "    \"My chest hurts when I breathe\",\n",
        "]\n",
        "\n",
        "for q in test_queries:\n",
        "    sem_brain, sem_domain, sem_conf, _ = smart_route(q)\n",
        "    hdc_brain, hdc_domain, hdc_conf, _ = hdc_router.route(q)\n",
        "    \n",
        "    sem_result = f\"{sem_domain} ({sem_conf:.2f})\"\n",
        "    hdc_result = f\"{hdc_domain} ({hdc_conf:.2f})\"\n",
        "    \n",
        "    match = \"‚úÖ\" if sem_domain == hdc_domain else \"‚ùå\"\n",
        "    print(f\"{q[:33]:<35} {sem_result:<15} {hdc_result:<15} {match}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
