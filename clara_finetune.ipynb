{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLARA FINE-TUNING NOTEBOOK - Google Colab Pro+\n",
        "\n",
        "**Instructions:**\n",
        "1. Upload this to Google Colab\n",
        "2. Set Runtime > Change runtime type > A100 GPU\n",
        "3. Run all cells\n",
        "4. Model saves to Google Drive automatically\n",
        "\n",
        "**Train each dimension separately by changing DIMENSION in Cell 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate wandb bitsandbytes\n",
        "!pip install -q peft trl\n",
        "!pip install -q sentencepiece\n",
        "\n",
        "import wandb\n",
        "wandb.login()  # Paste your WB API key when prompted\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"âœ“ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Configuration - EDIT THIS!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# WHICH DIMENSION/DOMAIN ARE YOU TRAINING?\n",
        "# ============================================================\n",
        "# Change this for each training run:\n",
        "#   Personality: warmth, playful, formal, encouragement\n",
        "#   Domain: medical, coding, teaching, quantum\n",
        "\n",
        "DIMENSION = \"warmth\"  # <-- CHANGE THIS EACH RUN\n",
        "\n",
        "# ============================================================\n",
        "# PATHS - Adjust if your data is in a different location\n",
        "# ============================================================\n",
        "# For personality data (warmth, playful, formal, encouragement):\n",
        "if DIMENSION in [\"warmth\", \"playful\", \"formal\", \"encouragement\"]:\n",
        "    DATA_PATH = f\"/content/drive/MyDrive/Lily/training_data/{DIMENSION}_training.json\"\n",
        "# For domain data (medical, coding, teaching, quantum):\n",
        "else:\n",
        "    DATA_PATH = f\"/content/drive/MyDrive/Lily/training_data/{DIMENSION}_knowledge.json\"\n",
        "\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/Lily/models/tinyllama_{DIMENSION}\"\n",
        "\n",
        "# ============================================================\n",
        "# BASE MODEL\n",
        "# ============================================================\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING CONFIG\n",
        "# ============================================================\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "print(f\"=\" * 60)\n",
        "print(f\"TRAINING: {DIMENSION}\")\n",
        "print(f\"=\" * 60)\n",
        "print(f\"Data: {DATA_PATH}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"Base model: {BASE_MODEL}\")\n",
        "print(f\"Epochs: {EPOCHS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected! Training will be very slow.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    print(f\"ERROR: Data file not found: {DATA_PATH}\")\n",
        "    print(\"\\nAvailable files in training_data:\")\n",
        "    training_dir = \"/content/drive/MyDrive/Lily/training_data\"\n",
        "    if os.path.exists(training_dir):\n",
        "        for f in os.listdir(training_dir):\n",
        "            print(f\"  - {f}\")\n",
        "    else:\n",
        "        print(f\"  Directory not found: {training_dir}\")\n",
        "    raise FileNotFoundError(f\"Data file not found: {DATA_PATH}\")\n",
        "\n",
        "# Load your training data\n",
        "with open(DATA_PATH) as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"âœ“ Loaded {len(raw_data)} examples\")\n",
        "print(f\"\\nSample example:\")\n",
        "print(json.dumps(raw_data[0], indent=2))\n",
        "\n",
        "# Format for training\n",
        "def format_training_data(examples, dimension):\n",
        "    \"\"\"Format data for instruction fine-tuning\"\"\"\n",
        "    formatted = []\n",
        "    \n",
        "    for ex in examples:\n",
        "        if 'neutral' in ex:  # Personality data format\n",
        "            # Train to transform neutral -> high\n",
        "            formatted.append({\n",
        "                \"instruction\": f\"Rewrite this with high {dimension}: {ex['neutral']}\",\n",
        "                \"response\": ex['high']\n",
        "            })\n",
        "            # Also train neutral -> low for contrast\n",
        "            formatted.append({\n",
        "                \"instruction\": f\"Rewrite this with low {dimension}: {ex['neutral']}\",\n",
        "                \"response\": ex['low']\n",
        "            })\n",
        "        elif 'question' in ex:  # Domain knowledge format\n",
        "            formatted.append({\n",
        "                \"instruction\": ex['question'],\n",
        "                \"response\": ex['answer']\n",
        "            })\n",
        "        else:\n",
        "            print(f\"Warning: Unknown format: {ex.keys()}\")\n",
        "    \n",
        "    return formatted\n",
        "\n",
        "formatted_data = format_training_data(raw_data, DIMENSION)\n",
        "print(f\"\\nâœ“ Formatted into {len(formatted_data)} training examples\")\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "print(f\"  Train: {len(dataset['train'])}\")\n",
        "print(f\"  Val: {len(dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Load Model with Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# 4-bit quantization for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(\"Loading model (this takes 1-2 minutes)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"âœ“ Model loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA = Low-Rank Adaptation\n",
        "# Trains only ~1% of parameters, much faster and uses less memory\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                # Rank of update matrices\n",
        "    lora_alpha=32,       # Scaling factor\n",
        "    target_modules=[     # Which layers to adapt\n",
        "        \"q_proj\", \n",
        "        \"k_proj\", \n",
        "        \"v_proj\", \n",
        "        \"o_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"âœ“ LoRA configured!\")\n",
        "print(f\"  Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt(instruction, response=\"\"):\n",
        "    \"\"\"Create training prompt in chat format\"\"\"\n",
        "    if response:\n",
        "        return f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "{response}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize examples for training\"\"\"\n",
        "    prompts = [\n",
        "        create_prompt(inst, resp) \n",
        "        for inst, resp in zip(examples['instruction'], examples['response'])\n",
        "    ]\n",
        "    \n",
        "    tokenized = tokenizer(\n",
        "        prompts,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "    \n",
        "    # Labels are same as input_ids for causal LM\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset['train'].column_names,\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "print(\"âœ“ Tokenization complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # Training\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
        "    \n",
        "    # Optimizer\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    \n",
        "    # Precision\n",
        "    fp16=True,\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,\n",
        "    \n",
        "    # Saving\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    \n",
        "    # Evaluation\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    \n",
        "    # W&B Integration\n",
        "    report_to=\"wandb\",\n",
        "    run_name=f\"clara-{DIMENSION}\",\n",
        "    \n",
        "    # Memory optimization\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=0.3,\n",
        ")\n",
        "\n",
        "print(\"âœ“ Training configuration ready!\")\n",
        "print(f\"\\n  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * 4})\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  W&B run: clara-{DIMENSION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling, Trainer\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # We're doing causal LM, not masked LM\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['test'],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"ðŸš€ STARTING TRAINING: {DIMENSION}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nWatch progress at:\")\n",
        "print(f\"  https://wandb.ai/chris_hartline/clara-deng-research\")\n",
        "print(f\"\\nEstimated time: 30-60 minutes on A100\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nâœ“ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nSaving model to: {OUTPUT_DIR}\")\n",
        "\n",
        "# Save the LoRA adapter\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Calculate size\n",
        "total_size = 0\n",
        "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
        "    for f in files:\n",
        "        total_size += os.path.getsize(os.path.join(root, f))\n",
        "\n",
        "print(f\"âœ“ Model saved!\")\n",
        "print(f\"  Size: {total_size / 1e6:.1f} MB\")\n",
        "print(f\"  Location: {OUTPUT_DIR}\")\n",
        "\n",
        "# List saved files\n",
        "print(f\"\\nSaved files:\")\n",
        "for f in os.listdir(OUTPUT_DIR):\n",
        "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f)) / 1e6\n",
        "    print(f\"  - {f} ({size:.1f} MB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING TRAINED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def generate_response(prompt, max_new_tokens=100):\n",
        "    \"\"\"Generate a response from the trained model\"\"\"\n",
        "    full_prompt = create_prompt(prompt)\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the response part\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.split(\"### Response:\")[-1].strip()\n",
        "    return response\n",
        "\n",
        "# Test with appropriate prompts based on dimension type\n",
        "if DIMENSION in [\"warmth\", \"playful\", \"formal\", \"encouragement\"]:\n",
        "    test_prompts = [\n",
        "        f\"Rewrite this with high {DIMENSION}: I can help you with that.\",\n",
        "        f\"Rewrite this with high {DIMENSION}: That's correct.\",\n",
        "        f\"Rewrite this with low {DIMENSION}: I'd be happy to assist you.\",\n",
        "    ]\n",
        "else:\n",
        "    test_prompts = [\n",
        "        f\"Explain a basic concept in {DIMENSION}.\",\n",
        "        f\"What is an important principle in {DIMENSION}?\",\n",
        "    ]\n",
        "\n",
        "print(\"\\nTest Results:\\n\")\n",
        "for prompt in test_prompts:\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Response: {response}\\n\")\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12: Done!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ“ TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\"\"\n",
        "Dimension trained: {DIMENSION}\n",
        "Model saved to: {OUTPUT_DIR}\n",
        "\n",
        "NEXT STEPS:\n",
        "===========\n",
        "\n",
        "1. To train another dimension:\n",
        "   - Change DIMENSION = \\\"{DIMENSION}\\\" to the next one\n",
        "   - Run all cells again\n",
        "   \n",
        "   Dimensions to train:\n",
        "   [ ] warmth\n",
        "   [ ] playful  \n",
        "   [ ] formal\n",
        "   [ ] encouragement\n",
        "   [ ] medical\n",
        "   [ ] coding\n",
        "   [ ] teaching\n",
        "   [ ] quantum\n",
        "\n",
        "2. After ALL dimensions are trained:\n",
        "   - Download models from Google Drive\n",
        "   - Run mergekit to combine them\n",
        "   - Create Clara!\n",
        "\n",
        "W&B Dashboard:\n",
        "  https://wandb.ai/chris_hartline/clara-deng-research\n",
        "\n",
        "Models location:\n",
        "  Google Drive/Lily/models/\n",
        "\"\"\")\n",
        "\n",
        "# Finish W&B run\n",
        "wandb.finish()\n",
        "\n",
        "print(\"âœ“ All done! Change DIMENSION and run again for the next one.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
